{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import os\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import timm\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = './data'\n",
    "\n",
    "train_file_path = f\"{work_dir}/Train.csv\"\n",
    "test_file_path = f\"{work_dir}/Test.csv\"\n",
    "\n",
    "\n",
    "FEAT_PATH_ = f'{work_dir}/Features_build'\n",
    "\n",
    "SEED_ = 42\n",
    "\n",
    "v1_models = [\n",
    "    ('efficientnet_b3.ra2_in1k',224), \n",
    "    ('resnet34',224),\n",
    "    ('vit_base_patch16_clip_224.openai', 224),\n",
    "    ('resnet50',224),\n",
    "    ('vit_large_patch14_reg4_dinov2.lvd142m',518),\n",
    "]\n",
    "\n",
    "v3_models = [\n",
    "    ('efficientnet_b3.ra2_in1k',224), \n",
    "    ('resnet34',224),\n",
    "    ('vit_base_patch16_clip_224.openai', 224),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Work On Images NZP File Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Start of Process & Usage of Images ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extractions Functions & Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Function For Transforming V1 & Extraction\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Constants and Basic Helper Functions\n",
    "# =============================================================================\n",
    "_MAX_INT = np.iinfo(np.uint16).max\n",
    "\n",
    "def decode_slope(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert 16-bit discretized slope to float32 radians.\"\"\"\n",
    "    return (x / _MAX_INT * (math.pi / 2.0)).astype(np.float32)\n",
    "\n",
    "def normalize(x: np.ndarray, mean: int, std: int) -> np.ndarray:\n",
    "    \"\"\"Normalize using (x - mean)/std.\"\"\"\n",
    "    return (x - mean) / std\n",
    "\n",
    "rough_S2_normalize = partial(normalize, mean=1250, std=500)\n",
    "\n",
    "def preprocess_image(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess composite image:\n",
    "      - If the image has a time series (ndim==4), average over time.\n",
    "      - Normalize the first five Sentinelâ€‘2 bands and decode the slope.\n",
    "    Assumes channels are:\n",
    "      0: B2 (Blue), 1: B3 (Green), 2: B4 (Red), 3: B8 (NIR),\n",
    "      4: B11 (SWIR), 5: slope.\n",
    "    \"\"\"\n",
    "    if x.ndim == 4:\n",
    "        x = x.mean(axis=0)\n",
    "    processed = np.concatenate([\n",
    "        rough_S2_normalize(x[..., :-1].astype(np.float32)),\n",
    "        decode_slope(x[..., -1:])\n",
    "    ], axis=-1).astype(np.float32)\n",
    "    return processed\n",
    "\n",
    "def compute_band_stats(image: np.ndarray) -> dict:\n",
    "    \"\"\"Compute per-band mean and standard deviation.\"\"\"\n",
    "    stats = {}\n",
    "    for ch in range(image.shape[-1]):\n",
    "        stats[f'band{ch}_mean'] = float(np.nanmean(image[..., ch]))\n",
    "        stats[f'band{ch}_std'] = float(np.nanstd(image[..., ch]))\n",
    "    return stats\n",
    "\n",
    "def compute_indices(image: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Compute NDVI and NDMI (here called endmi) indices.\n",
    "    For the image:\n",
    "      - Red (B4) is channel 2\n",
    "      - NIR (B8) is channel 3\n",
    "      - SWIR (B11) is channel 4\n",
    "    \"\"\"\n",
    "    B4 = image[..., 2]\n",
    "    B8 = image[..., 3]\n",
    "    B11 = image[..., 4]\n",
    "    ndvi_den = (B8 + B4)\n",
    "    ndvi = np.where(ndvi_den != 0, (B8 - B4) / ndvi_den, 0)\n",
    "    ndmi_den = (B8 + B11)\n",
    "    ndmi = np.where(ndmi_den != 0, (B8 - B11) / ndmi_den, 0)\n",
    "    return {\n",
    "        'ndvi_mean': float(np.nanmean(ndvi)),\n",
    "        'ndvi_std': float(np.nanstd(ndvi)),\n",
    "        'endmi_mean': float(np.nanmean(ndmi)),\n",
    "        'endmi_std': float(np.nanstd(ndmi)),\n",
    "    }\n",
    "\n",
    "def extract_natural_color(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract a natural color composite from the processed image.\n",
    "    Uses:\n",
    "      - Red   = B4 (channel 2)\n",
    "      - Green = B3 (channel 1)\n",
    "      - Blue  = B2 (channel 0)\n",
    "    \"\"\"\n",
    "    return image[..., [2, 1, 0]]\n",
    "\n",
    "def normalize_to_uint8(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize a 2D or 3D image to [0, 255] as uint8 (done per channel).\n",
    "    \"\"\"\n",
    "    if img.ndim == 2:\n",
    "        min_val, max_val = img.min(), img.max()\n",
    "        if max_val - min_val < 1e-6:\n",
    "            return np.zeros_like(img, dtype=np.uint8)\n",
    "        norm = (img - min_val) / (max_val - min_val)\n",
    "        return (norm * 255).astype(np.uint8)\n",
    "    elif img.ndim == 3:\n",
    "        out = []\n",
    "        for c in range(img.shape[-1]):\n",
    "            channel = img[..., c]\n",
    "            min_val, max_val = channel.min(), channel.max()\n",
    "            if max_val - min_val < 1e-6:\n",
    "                norm = np.zeros_like(channel)\n",
    "            else:\n",
    "                norm = (channel - min_val) / (max_val - min_val)\n",
    "            out.append((norm * 255).astype(np.uint8))\n",
    "        return np.stack(out, axis=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image dimension.\")\n",
    "\n",
    "def get_timm_features(pil_img: Image.Image, transform, model, device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract features from a PIL image using the given TIMM model.\n",
    "    Returns a flattened feature vector.\n",
    "    \"\"\"\n",
    "    input_tensor = transform(pil_img).unsqueeze(0).to(device)  # shape: (1, 3, H, W)\n",
    "    with torch.no_grad():\n",
    "        features = model(input_tensor)\n",
    "    return features.cpu().numpy().flatten()\n",
    "\n",
    "def compute_additional_band_stats(image: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Compute extra statistics (skewness and kurtosis) per band.\n",
    "    These additional features can capture distribution characteristics.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for ch in range(image.shape[-1]):\n",
    "        stats[f'band{ch}_skew'] = float(skew(image[..., ch].ravel()))\n",
    "        stats[f'band{ch}_kurtosis'] = float(kurtosis(image[..., ch].ravel()))\n",
    "    return stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Function For Transforming V3\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Debug Function for Images\n",
    "# =============================================================================\n",
    "def debug_images():\n",
    "    \"\"\"\n",
    "    Debug function to display one sample event's images.\n",
    "    Displays:\n",
    "      - Raw image (natural-color composite)\n",
    "      - Processed image (after normalization and slope decoding)\n",
    "      - Final image (8-bit RGB for viewing)\n",
    "    Also prints out shapes.\n",
    "    \"\"\"\n",
    "    BASE_PATH = './data'\n",
    "    COMPOSITE_IMAGES_PATH = os.path.join(BASE_PATH, 'composite_images.npz')\n",
    "    images = np.load(COMPOSITE_IMAGES_PATH)\n",
    "    \n",
    "    sample_event = list(images.keys())[0]\n",
    "    raw_img = images[sample_event]\n",
    "    \n",
    "    print(f\"[DEBUG] Sample event_id: {sample_event}\")\n",
    "    print(\"[DEBUG] Raw image shape:\", raw_img.shape)\n",
    "    \n",
    "    if raw_img.ndim == 4:\n",
    "        print(\"[DEBUG] Image is a time series. Using the first snapshot.\")\n",
    "        snapshot = raw_img[0]\n",
    "        print(\"[DEBUG] Snapshot shape:\", snapshot.shape)\n",
    "    elif raw_img.ndim == 3:\n",
    "        print(\"[DEBUG] Image is static.\")\n",
    "        snapshot = raw_img\n",
    "    else:\n",
    "        print(\"[DEBUG] Unexpected image dimensions:\", raw_img.ndim)\n",
    "        return\n",
    "\n",
    "    if snapshot.shape[-1] >= 3:\n",
    "        raw_rgb = snapshot[..., [2, 1, 0]]\n",
    "        raw_rgb_norm = (raw_rgb - raw_rgb.min()) / (raw_rgb.max() - raw_rgb.min() + 1e-6)\n",
    "    else:\n",
    "        raw_rgb_norm = snapshot\n",
    "\n",
    "    proc_img = preprocess_image(snapshot)\n",
    "    print(\"[DEBUG] Processed image shape:\", proc_img.shape)\n",
    "    proc_rgb = extract_natural_color(proc_img)\n",
    "    proc_rgb_norm = (proc_rgb - proc_rgb.min()) / (proc_rgb.max() - proc_rgb.min() + 1e-6)\n",
    "    final_img = normalize_to_uint8(proc_rgb)\n",
    "    print(\"[DEBUG] Final image shape (uint8):\", final_img.shape)\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(raw_rgb_norm)\n",
    "    plt.title(\"Raw Image (Natural Color Composite)\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(proc_rgb_norm)\n",
    "    plt.title(\"Processed Image (Normalized)\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(final_img)\n",
    "    plt.title(\"Final Image (8-bit RGB)\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# Autoencoder for TIMM Feature Reduction\n",
    "# =============================================================================\n",
    "class FeatureAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(FeatureAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return z, x_recon\n",
    "\n",
    "def train_feature_autoencoder(features, latent_dim, epochs, lr, device):\n",
    "    \"\"\"\n",
    "    Train an autoencoder on the TIMM features.\n",
    "    Returns the encoded (latent) features.\n",
    "    \"\"\"\n",
    "    input_dim = features.shape[1]\n",
    "    model = FeatureAutoencoder(input_dim, latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        z, x_recon = model(features_tensor)\n",
    "        loss = loss_fn(x_recon, features_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[AUTOENCODER] Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z, _ = model(features_tensor)\n",
    "    return z.cpu().numpy()\n",
    "\n",
    "# =============================================================================\n",
    "# Attention Fusion Autoencoder for Multimodal Fusion\n",
    "# =============================================================================\n",
    "class AttentionFusionAE(nn.Module):\n",
    "    def __init__(self, img_dim, tab_dim, fusion_dim):\n",
    "        super(AttentionFusionAE, self).__init__()\n",
    "        self.fc_img = nn.Linear(img_dim, fusion_dim)\n",
    "        self.fc_tab = nn.Linear(tab_dim, fusion_dim)\n",
    "        self.attn = nn.Linear(fusion_dim, 1)\n",
    "        # Decoders for reconstruction (unsupervised training objective)\n",
    "        self.decoder_img = nn.Linear(fusion_dim, img_dim)\n",
    "        self.decoder_tab = nn.Linear(fusion_dim, tab_dim)\n",
    "    def forward(self, img_feat, tab_feat):\n",
    "        img_proj = self.fc_img(img_feat)\n",
    "        tab_proj = self.fc_tab(tab_feat)\n",
    "        fusion = torch.tanh(img_proj + tab_proj)\n",
    "        weight = torch.sigmoid(self.attn(fusion))  # (N, 1)\n",
    "        fused = weight * img_proj + (1 - weight) * tab_proj  # (N, fusion_dim)\n",
    "        rec_img = self.decoder_img(fused)\n",
    "        rec_tab = self.decoder_tab(fused)\n",
    "        return fused, rec_img, rec_tab\n",
    "\n",
    "def train_attention_fusion(img_feats, tab_feats, fusion_dim, epochs, lr, device):\n",
    "    \"\"\"\n",
    "    Train an attention fusion autoencoder on image and tabular features.\n",
    "    Returns the fused features.\n",
    "    \"\"\"\n",
    "    img_dim = img_feats.shape[1]\n",
    "    tab_dim = tab_feats.shape[1]\n",
    "    model = AttentionFusionAE(img_dim, tab_dim, fusion_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    img_tensor = torch.tensor(img_feats, dtype=torch.float32).to(device)\n",
    "    tab_tensor = torch.tensor(tab_feats, dtype=torch.float32).to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        fused, rec_img, rec_tab = model(img_tensor, tab_tensor)\n",
    "        loss_img = loss_fn(rec_img, img_tensor)\n",
    "        loss_tab = loss_fn(rec_tab, tab_tensor)\n",
    "        loss = loss_img + loss_tab\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[FUSION AE] Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        fused, _, _ = model(img_tensor, tab_tensor)\n",
    "    return fused.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipelineV1(debug=False, img_dimension =224, MODEL_NAME = None):\n",
    "    \"\"\"\n",
    "    Run the full feature-engineering pipeline.\n",
    "    \n",
    "    If debug=True, prints detailed information about:\n",
    "      - Data shapes (raw, processed, features)\n",
    "      - Model name and TIMM feature vector shape per event\n",
    "      - PCA output shapes for each setting\n",
    "      - Final dataframe shape and column names\n",
    "      - Also displays image diagnostics via debug_images()\n",
    "    \n",
    "    Returns:\n",
    "        df_features: A pandas DataFrame with one row per event and all engineered features.\n",
    "    \"\"\"\n",
    "    BASE_PATH = work_dir #'./data'\n",
    "    TRAIN_CSV = os.path.join(BASE_PATH, 'Train.csv')\n",
    "    TEST_CSV = os.path.join(BASE_PATH, 'Test.csv')\n",
    "    COMPOSITE_IMAGES_PATH = os.path.join(BASE_PATH, 'composite_images.npz')\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Load CSV files and extract event IDs\n",
    "    # ------------------------------\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    train_df['event_id'] = train_df['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "    test_df['event_id'] = test_df['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "    train_event_ids = train_df['event_id'].unique().tolist()\n",
    "    test_event_ids = test_df['event_id'].unique().tolist()\n",
    "    all_event_ids = list(set(train_event_ids) | set(test_event_ids))\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Total unique event ids: {len(all_event_ids)}\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Load Composite Images\n",
    "    # ------------------------------\n",
    "    images = np.load(COMPOSITE_IMAGES_PATH)\n",
    "    sample_key = list(images.keys())[0]\n",
    "    sample_img = images[sample_key]\n",
    "    if sample_img.ndim == 4:\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Composite images have a time series dimension; averaging over time.\")\n",
    "    elif sample_img.ndim == 3:\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Composite images are static (one frame per event).\")\n",
    "    else:\n",
    "        print(\"[DEBUG] Unexpected image dimensions:\", sample_img.ndim)\n",
    "        return\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Set up device and TIMM model\n",
    "    # ------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    timm_model_name = MODEL_NAME\n",
    "    timm_model = timm.create_model(timm_model_name, pretrained=True, num_classes=0)\n",
    "    timm_model.to(device)\n",
    "    timm_model.eval()\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] TIMM Model: {timm_model_name} (classifier removed)\")\n",
    "    \n",
    "    transform = create_transform(\n",
    "        input_size=img_dimension,\n",
    "        is_training=False,\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Loop over events to compute features\n",
    "    # ------------------------------\n",
    "    features_dict = {}\n",
    "    timm_features_list = []\n",
    "    event_ids_list = []\n",
    "    \n",
    "    for event_id in tqdm(all_event_ids, desc=\"Processing events\"):\n",
    "        if event_id not in images:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Warning: event_id {event_id} not found in composite images. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        raw_img = images[event_id]\n",
    "        proc_img = preprocess_image(raw_img)\n",
    "        \n",
    "        # Compute hand-engineered features.\n",
    "        band_stats = compute_band_stats(proc_img)\n",
    "        indices_stats = compute_indices(proc_img)\n",
    "        \n",
    "        # Prepare natural-color image for TIMM extraction.\n",
    "        natural_img = extract_natural_color(proc_img)\n",
    "        natural_img_uint8 = normalize_to_uint8(natural_img)\n",
    "        pil_img = Image.fromarray(natural_img_uint8)\n",
    "        \n",
    "        # Extract TIMM features.\n",
    "        timm_feat = get_timm_features(pil_img, transform, timm_model, device)\n",
    "        # if debug:\n",
    "        #     print(f\"[DEBUG] Event {event_id}: processed image shape {proc_img.shape}, TIMM feature vector shape {timm_feat.shape}\")\n",
    "        \n",
    "        feats = {}\n",
    "        feats.update(band_stats)\n",
    "        feats.update(indices_stats)\n",
    "        feats['timm_feat_dim'] = len(timm_feat)\n",
    "        features_dict[event_id] = feats\n",
    "        timm_features_list.append(timm_feat)\n",
    "        event_ids_list.append(event_id)\n",
    "    \n",
    "    df_features = pd.DataFrame.from_dict(features_dict, orient='index')\n",
    "    df_features.index.name = 'event_id'\n",
    "    \n",
    "    timm_features_matrix = np.vstack(timm_features_list)\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] TIMM features matrix shape: {timm_features_matrix.shape}\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # PCA on TIMM features and append PCA columns\n",
    "    # ------------------------------\n",
    "    for n_components in [16, 32, 64, 128, 256]:\n",
    "        pca = PCA(n_components=n_components, random_state = SEED_)\n",
    "        pca_components = pca.fit_transform(timm_features_matrix)\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] PCA with {n_components} components, output shape: {pca_components.shape}\")\n",
    "        for i in range(n_components):\n",
    "            col_name = f'pca{n_components}_{i}'\n",
    "            df_features[col_name] = pd.Series(pca_components[:, i], index=event_ids_list)\n",
    "    \n",
    "    df_features.reset_index(inplace=True)\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Final dataframe shape: {df_features.shape}\")\n",
    "        print(f\"[DEBUG] Dataframe columns: {df_features.columns.tolist()}\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # If debugging, display sample images and diagnostics.\n",
    "    # ------------------------------\n",
    "    if debug:\n",
    "        debug_images()\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_pipelineV3(debug=False,\n",
    "                 keep_original_image_features=True,\n",
    "                 target_dim=100,\n",
    "                 extra_image_features=False,\n",
    "                 use_autoencoder=False,\n",
    "                 autoencoder_epochs=50,\n",
    "                 autoencoder_lr=1e-3,\n",
    "                 use_attention_fusion=False,\n",
    "                 fusion_dim=128,\n",
    "                 fusion_epochs=50,\n",
    "                 fusion_lr=1e-3,\n",
    "                 img_dimension = 224,\n",
    "                 MODEL_NAME = None\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Run the full feature-engineering pipeline with several toggles.\n",
    "    \n",
    "    Parameters:\n",
    "      - debug: Print detailed debug information and display images.\n",
    "      - keep_original_image_features: If True, keep full TIMM features; if False, reduce them.\n",
    "      - target_dim: Target dimension for TIMM feature reduction (via PCA or autoencoder).\n",
    "      - extra_image_features: Compute extra band statistics (skewness, kurtosis).\n",
    "      - use_autoencoder: If True, use a trainable autoencoder to reduce TIMM features.\n",
    "      - autoencoder_epochs, autoencoder_lr: Hyperparameters for the autoencoder.\n",
    "      - use_attention_fusion: If True, fuse tabular (hand-engineered) and deep image features via an attention autoencoder.\n",
    "      - fusion_dim, fusion_epochs, fusion_lr: Hyperparameters for the attention fusion autoencoder.\n",
    "    \n",
    "    Returns:\n",
    "        df_features: A pandas DataFrame with one row per event containing all engineered features.\n",
    "    \"\"\"\n",
    "    BASE_PATH = work_dir #'./data'\n",
    "    TRAIN_CSV = os.path.join(BASE_PATH, 'Train.csv')\n",
    "    TEST_CSV = os.path.join(BASE_PATH, 'Test.csv')\n",
    "    COMPOSITE_IMAGES_PATH = os.path.join(BASE_PATH, 'composite_images.npz')\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Load CSVs and extract event IDs\n",
    "    # ------------------------------\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    train_df['event_id'] = train_df['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "    test_df['event_id'] = test_df['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "    train_event_ids = train_df['event_id'].unique().tolist()\n",
    "    test_event_ids = test_df['event_id'].unique().tolist()\n",
    "    all_event_ids = list(set(train_event_ids) | set(test_event_ids))\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Total unique event ids: {len(all_event_ids)}\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Load Composite Images\n",
    "    # ------------------------------\n",
    "    images = np.load(COMPOSITE_IMAGES_PATH)\n",
    "    sample_key = list(images.keys())[0]\n",
    "    sample_img = images[sample_key]\n",
    "    if sample_img.ndim == 4:\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Composite images have a time series dimension; averaging over time.\")\n",
    "    elif sample_img.ndim == 3:\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Composite images are static (one frame per event).\")\n",
    "    else:\n",
    "        print(\"[DEBUG] Unexpected image dimensions:\", sample_img.ndim)\n",
    "        return\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Set up device and TIMM model\n",
    "    # ------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    timm_model_name = MODEL_NAME # \"resnet50\"\n",
    "    timm_model = timm.create_model(timm_model_name, pretrained=True, num_classes=0)\n",
    "    timm_model.to(device)\n",
    "    timm_model.eval()\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] TIMM Model: {timm_model_name} (classifier removed)\")\n",
    "    \n",
    "    transform = create_transform(\n",
    "        input_size=img_dimension,\n",
    "        is_training=False,\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Loop over events to compute features\n",
    "    # ------------------------------\n",
    "    features_dict = {}\n",
    "    timm_features_list = []\n",
    "    event_ids_list = []\n",
    "    \n",
    "    for event_id in tqdm(all_event_ids, desc=\"Processing events\"):\n",
    "        if event_id not in images:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Warning: event_id {event_id} not found in composite images. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        raw_img = images[event_id]\n",
    "        proc_img = preprocess_image(raw_img)\n",
    "        \n",
    "        # Compute basic hand-engineered features.\n",
    "        feats = {}\n",
    "        feats.update(compute_band_stats(proc_img))\n",
    "        feats.update(compute_indices(proc_img))\n",
    "        if extra_image_features:\n",
    "            feats.update(compute_additional_band_stats(proc_img))\n",
    "        \n",
    "        # Prepare natural-color image for TIMM extraction.\n",
    "        natural_img = extract_natural_color(proc_img)\n",
    "        natural_img_uint8 = normalize_to_uint8(natural_img)\n",
    "        pil_img = Image.fromarray(natural_img_uint8)\n",
    "        \n",
    "        # Extract TIMM features.\n",
    "        timm_feat = get_timm_features(pil_img, transform, timm_model, device)\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Event {event_id}: processed image shape {proc_img.shape}, TIMM feature vector shape {timm_feat.shape}\")\n",
    "        \n",
    "        feats['timm_feat_dim'] = len(timm_feat)\n",
    "        features_dict[event_id] = feats\n",
    "        timm_features_list.append(timm_feat)\n",
    "        event_ids_list.append(event_id)\n",
    "    \n",
    "    # Create dataframe from hand-engineered features.\n",
    "    df_features = pd.DataFrame.from_dict(features_dict, orient='index')\n",
    "    df_features.index.name = 'event_id'\n",
    "    \n",
    "    timm_features_matrix = np.vstack(timm_features_list)\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] TIMM features matrix shape: {timm_features_matrix.shape}\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Process TIMM features: Either keep, reduce via PCA, or via Autoencoder.\n",
    "    # ------------------------------\n",
    "    if keep_original_image_features:\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Keeping original TIMM features.\")\n",
    "        for i in range(timm_features_matrix.shape[1]):\n",
    "            col_name = f'timm_orig_{i}'\n",
    "            df_features[col_name] = pd.Series(timm_features_matrix[:, i], index=df_features.index)\n",
    "        deep_features = timm_features_matrix  # for later fusion if needed\n",
    "    else:\n",
    "        if use_autoencoder:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Reducing TIMM features using Autoencoder to {target_dim} dims over {autoencoder_epochs} epochs.\")\n",
    "            reduced = train_feature_autoencoder(timm_features_matrix, target_dim, autoencoder_epochs, autoencoder_lr, device)\n",
    "            for i in range(reduced.shape[1]):\n",
    "                col_name = f'timm_autoencoder_{i}'\n",
    "                df_features[col_name] = pd.Series(reduced[:, i], index=df_features.index)\n",
    "            deep_features = reduced\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Reducing TIMM features using PCA to {target_dim} dims.\")\n",
    "            pca = PCA(n_components=target_dim)\n",
    "            reduced = pca.fit_transform(timm_features_matrix)\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] PCA reduced features shape: {reduced.shape}\")\n",
    "            for i in range(reduced.shape[1]):\n",
    "                col_name = f'timm_pca_{i}'\n",
    "                df_features[col_name] = pd.Series(reduced[:, i], index=df_features.index)\n",
    "            deep_features = reduced\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Optional Attention Fusion between deep and tabular features.\n",
    "    # ------------------------------\n",
    "    if use_attention_fusion:\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Applying Attention Fusion with fusion dimension: {fusion_dim} over {fusion_epochs} epochs.\")\n",
    "        all_cols = df_features.columns.tolist()\n",
    "        deep_feature_cols = [col for col in all_cols if col.startswith('timm_')]\n",
    "        tabular_feature_cols = [col for col in all_cols if col not in deep_feature_cols]\n",
    "        img_feats = df_features[deep_feature_cols].values\n",
    "        tab_feats = df_features[tabular_feature_cols].values\n",
    "        fused_features = train_attention_fusion(img_feats, tab_feats, fusion_dim, fusion_epochs, fusion_lr, device)\n",
    "        for i in range(fused_features.shape[1]):\n",
    "            col_name = f'fusion_{i}'\n",
    "            df_features[col_name] = pd.Series(fused_features[:, i], index=df_features.index)\n",
    "    \n",
    "    # Optionally reset index to include event_id as a column.\n",
    "    df_features.reset_index(inplace=True)\n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Final dataframe shape: {df_features.shape}\")\n",
    "        print(f\"[DEBUG] Dataframe columns (first 10): {df_features.columns.tolist()[:10]}\")\n",
    "    \n",
    "    if debug:\n",
    "        debug_images()\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute & Save Features Extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511ff2c30a9a419690d83aae09f48eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898/898 [00:14<00:00, 59.98it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e459a5b8e4b64227912967dc102a1d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898/898 [00:08<00:00, 110.81it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c016ff039374fee97ca904348e8840c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898/898 [00:09<00:00, 95.29it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e82b2371744e8b870587e811bfd358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898/898 [00:09<00:00, 94.86it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad72b04688a4a439e58d95fb8bb79ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898/898 [01:48<00:00,  8.24it/s]\n",
      "Processing events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898/898 [00:19<00:00, 45.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUTOENCODER] Epoch 0/110, Loss: 0.0684\n",
      "[AUTOENCODER] Epoch 10/110, Loss: 0.0438\n",
      "[AUTOENCODER] Epoch 20/110, Loss: 0.0334\n",
      "[AUTOENCODER] Epoch 30/110, Loss: 0.0265\n",
      "[AUTOENCODER] Epoch 40/110, Loss: 0.0219\n",
      "[AUTOENCODER] Epoch 50/110, Loss: 0.0184\n",
      "[AUTOENCODER] Epoch 60/110, Loss: 0.0162\n",
      "[AUTOENCODER] Epoch 70/110, Loss: 0.0136\n",
      "[AUTOENCODER] Epoch 80/110, Loss: 0.0119\n",
      "[AUTOENCODER] Epoch 90/110, Loss: 0.0105\n",
      "[AUTOENCODER] Epoch 100/110, Loss: 0.0093\n",
      "[FUSION AE] Epoch 0/50, Loss: 79554.2734\n",
      "[FUSION AE] Epoch 10/50, Loss: 56161.3906\n",
      "[FUSION AE] Epoch 20/50, Loss: 26546.5000\n",
      "[FUSION AE] Epoch 30/50, Loss: 14188.9590\n",
      "[FUSION AE] Epoch 40/50, Loss: 9252.1924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898/898 [00:13<00:00, 68.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUTOENCODER] Epoch 0/110, Loss: 0.0655\n",
      "[AUTOENCODER] Epoch 10/110, Loss: 0.0273\n",
      "[AUTOENCODER] Epoch 20/110, Loss: 0.0223\n",
      "[AUTOENCODER] Epoch 30/110, Loss: 0.0195\n",
      "[AUTOENCODER] Epoch 40/110, Loss: 0.0168\n",
      "[AUTOENCODER] Epoch 50/110, Loss: 0.0147\n",
      "[AUTOENCODER] Epoch 60/110, Loss: 0.0127\n",
      "[AUTOENCODER] Epoch 70/110, Loss: 0.0112\n",
      "[AUTOENCODER] Epoch 80/110, Loss: 0.0098\n",
      "[AUTOENCODER] Epoch 90/110, Loss: 0.0088\n",
      "[AUTOENCODER] Epoch 100/110, Loss: 0.0080\n",
      "[FUSION AE] Epoch 0/50, Loss: 58070.3438\n",
      "[FUSION AE] Epoch 10/50, Loss: 41333.2930\n",
      "[FUSION AE] Epoch 20/50, Loss: 11938.2627\n",
      "[FUSION AE] Epoch 30/50, Loss: 2363.0186\n",
      "[FUSION AE] Epoch 40/50, Loss: 2543.5928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898/898 [00:14<00:00, 60.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUTOENCODER] Epoch 0/110, Loss: 0.8750\n",
      "[AUTOENCODER] Epoch 10/110, Loss: 0.2044\n",
      "[AUTOENCODER] Epoch 20/110, Loss: 0.1511\n",
      "[AUTOENCODER] Epoch 30/110, Loss: 0.1294\n",
      "[AUTOENCODER] Epoch 40/110, Loss: 0.1154\n",
      "[AUTOENCODER] Epoch 50/110, Loss: 0.1037\n",
      "[AUTOENCODER] Epoch 60/110, Loss: 0.0935\n",
      "[AUTOENCODER] Epoch 70/110, Loss: 0.0838\n",
      "[AUTOENCODER] Epoch 80/110, Loss: 0.0752\n",
      "[AUTOENCODER] Epoch 90/110, Loss: 0.0684\n",
      "[AUTOENCODER] Epoch 100/110, Loss: 0.0630\n",
      "[FUSION AE] Epoch 0/50, Loss: 64260.3672\n",
      "[FUSION AE] Epoch 10/50, Loss: 37654.0078\n",
      "[FUSION AE] Epoch 20/50, Loss: 11885.1826\n",
      "[FUSION AE] Epoch 30/50, Loss: 4690.3169\n",
      "[FUSION AE] Epoch 40/50, Loss: 4127.8643\n"
     ]
    }
   ],
   "source": [
    "for (model_, img_) in v1_models:\n",
    "    df = run_pipelineV1(debug=False, img_dimension = img_, MODEL_NAME = model_ )\n",
    "    df.to_parquet(f'{FEAT_PATH_}/Processed_V1_{model_}_S{SEED_}.parquet', engine='pyarrow')\n",
    "\n",
    "for (model_, img_) in v3_models:\n",
    "    df = run_pipelineV3(debug=False,\n",
    "                keep_original_image_features=False,  # reduce deep features\n",
    "                target_dim=100,\n",
    "                extra_image_features=True,\n",
    "                use_autoencoder=True,     # use autoencoder reduction instead of PCA\n",
    "                autoencoder_epochs=110,\n",
    "                autoencoder_lr=1e-3,\n",
    "                use_attention_fusion=True,  # enable attention fusion of deep and tabular features\n",
    "                fusion_dim=128,\n",
    "                fusion_epochs=50,\n",
    "                fusion_lr=1e-3, img_dimension = img_, MODEL_NAME = model_)\n",
    "    df.to_parquet(f'{FEAT_PATH_}/Processed_V3_{model_}_S{SEED_}.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Processed Features to Single DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_parquet_files(directory='./data/Features_build', zero_only=True):\n",
    "    \"\"\"\n",
    "    Reads all parquet files in `directory` whose filenames start with \"Processed_V\",\n",
    "    renames their columns (except 'event_id') by appending a file-specific suffix,\n",
    "    and merges them on 'event_id'.\n",
    "\n",
    "    If zero_only is True, the function filters out PCA columns whose original \n",
    "    component (before adding the file suffix) is not \"0\". For PCA columns with a\n",
    "    component of \"0\", it removes the trailing \"_0\" and file suffix.\n",
    "\n",
    "    Parameters:\n",
    "      - directory (str): Directory to search for parquet files.\n",
    "      - zero_only (bool): If True, only PCA columns ending with component \"0\" are kept.\n",
    "    \n",
    "    Returns:\n",
    "      - merged_df (pd.DataFrame): The merged (and possibly filtered) DataFrame.\n",
    "    \"\"\"\n",
    "    # Find all files matching the pattern (e.g., ProcessV1_*.parquet)\n",
    "    file_pattern = os.path.join(directory, 'Processed_V1_*.parquet')\n",
    "    files = glob.glob(file_pattern)\n",
    "    if not files:\n",
    "        print(\"No files found matching the pattern.\")\n",
    "        return None\n",
    "\n",
    "    merged_df = None\n",
    "\n",
    "    for file_path in files:\n",
    "        # Extract file-specific suffix:\n",
    "        # Example: \"ProcessV1_A.parquet\" -> suffix \"A\"\n",
    "        base_name = os.path.basename(file_path)\n",
    "        suffix = base_name[len(\"Processed_V1_\"):-len(\".parquet\")]\n",
    "\n",
    "        # Read the parquet file.\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Rename all columns (except 'event_id') by appending the file-specific suffix.\n",
    "        df = df.rename(columns={\n",
    "            col: col if col == 'event_id' else f\"{col}_{suffix}\"\n",
    "            for col in df.columns\n",
    "        })\n",
    "\n",
    "        # Merge sequentially on 'event_id'\n",
    "        if merged_df is None:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on='event_id', how='outer')\n",
    "\n",
    "    # --- Filtering PCA columns if zero_only is True ---\n",
    "    if zero_only:\n",
    "        drop_cols = []\n",
    "        rename_map = {}\n",
    "\n",
    "        # Process each column in the merged dataframe.\n",
    "        for col in merged_df.columns:\n",
    "            # Check in a case-insensitive manner if the column is a PCA column.\n",
    "            if col.lower().startswith(\"pca\"):\n",
    "                # We assume the structure is: <base>_<component>_<file_suffix>\n",
    "                # For example:\n",
    "                #   Original column: \"pca256_251\" in file, becomes \"pca256_251_resnet34\" after renaming.\n",
    "                #   We want to keep only those where the component is \"0\", e.g. \"pca256_0_resnet34\"\n",
    "                #\n",
    "                # First, remove the file-specific suffix by splitting on the last underscore.\n",
    "                if '_' not in col:\n",
    "                    continue  # Skip unexpected format\n",
    "                try:\n",
    "                    original_part, file_suffix = col.rsplit('_', 1)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                # Next, split original_part on its last underscore to get the component.\n",
    "                if '_' not in original_part:\n",
    "                    continue\n",
    "                try:\n",
    "                    base_feature, component = original_part.rsplit('_', 1)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                # If the component is not \"0\", mark this column for dropping.\n",
    "                if component != \"0\":\n",
    "                    drop_cols.append(col)\n",
    "                else:\n",
    "                    # For PCA columns with component \"0\", rename the column to the base feature.\n",
    "                    # For example: \"pca256_0_resnet34\" becomes \"pca256\"\n",
    "                    rename_map[col] = f'{base_feature}_CL'\n",
    "\n",
    "        # Drop all PCA columns that don't have component \"0\"\n",
    "        if drop_cols:\n",
    "            merged_df.drop(columns=drop_cols, inplace=True)\n",
    "        # Rename the kept PCA columns\n",
    "        if rename_map:\n",
    "            merged_df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def merge_processv3_files(directory='./data/Features_build'):\n",
    "    \"\"\"\n",
    "    Reads all parquet files in `directory` whose filenames start with \"Processed_V3\",\n",
    "    renames their columns (except 'event_id') by appending a file-specific suffix,\n",
    "    and merges them on 'event_id' (outer join).\n",
    "\n",
    "    Unlike the ProcessV1 merge, this function does not apply any PCA filtering, so all\n",
    "    columns from the ProcessV3 files are retained.\n",
    "\n",
    "    Parameters:\n",
    "      directory (str): Directory to search for parquet files.\n",
    "    \n",
    "    Returns:\n",
    "      merged_df (pd.DataFrame): The merged DataFrame containing all columns.\n",
    "    \"\"\"\n",
    "    # Find all ProcessV3 files.\n",
    "    file_pattern = os.path.join(directory, 'Processed_V3_*.parquet')\n",
    "    files = glob.glob(file_pattern)\n",
    "    if not files:\n",
    "        print(\"No ProcessV3 files found matching the pattern.\")\n",
    "        return None\n",
    "\n",
    "    merged_df = None\n",
    "\n",
    "    for file_path in files:\n",
    "        # Extract the file-specific suffix.\n",
    "        # Example: \"ProcessV3_modelA.parquet\" yields suffix \"modelA\"\n",
    "        base_name = os.path.basename(file_path)\n",
    "        suffix = base_name[len(\"Processed_V3_\"):-len(\".parquet\")]\n",
    "\n",
    "        # Read the parquet file.\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Rename all columns (except 'event_id') by appending the file-specific suffix.\n",
    "        df = df.rename(columns={\n",
    "            col: col if col == 'event_id' else f\"{col}_{suffix}\"\n",
    "            for col in df.columns\n",
    "        })\n",
    "\n",
    "        # Merge sequentially on 'event_id' using an outer join.\n",
    "        if merged_df is None:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on='event_id', how='outer')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def remove_duplicate_merge_columns(df, suffix_x='_x', suffix_y='_y'):\n",
    "    \"\"\"\n",
    "    Given a DataFrame resulting from a merge that might contain duplicate columns\n",
    "    with suffixes (e.g. _x and _y), this function checks for matching pairs.\n",
    "    If the values in the pair are identical, it drops the _y column and renames the\n",
    "    _x column to remove the suffix.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        suffix_x (str): Suffix for columns from the left DataFrame (default '_x').\n",
    "        suffix_y (str): Suffix for columns from the right DataFrame (default '_y').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed DataFrame with duplicate columns removed.\n",
    "    \"\"\"\n",
    "    # We make a list of column names to iterate over because we may change the DataFrame during iteration.\n",
    "    for col in list(df.columns):\n",
    "        if col.endswith(suffix_x):\n",
    "            # Get the base name by removing the _x suffix.\n",
    "            base_name = col[:-len(suffix_x)]\n",
    "            col_y = base_name + suffix_y\n",
    "            if col_y in df.columns:\n",
    "                # Check if the two columns are identical\n",
    "                if df[col].equals(df[col_y]):\n",
    "                    # Drop the _y column\n",
    "                    df.drop(columns=[col_y], inplace=True)\n",
    "                    # Rename the _x column to remove the suffix so that it has the base name.\n",
    "                    df.rename(columns={col: base_name}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def drop_or_rename_duplicate_columns_all(df, rename_suffix='_dup'):\n",
    "    \"\"\"\n",
    "    Processes duplicate columns in a DataFrame.\n",
    "    \n",
    "    For each set of duplicate columns (i.e. columns with the same name):\n",
    "      - The first occurrence is kept unchanged.\n",
    "      - For any subsequent duplicate column:\n",
    "          - If its values are identical to the first occurrence, it is dropped.\n",
    "          - Otherwise, it is renamed by appending a suffix (default: '_dup') plus an incrementing counter.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): The input DataFrame that may contain duplicate column names.\n",
    "      rename_suffix (str): Suffix used when renaming a duplicate column whose values differ.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: A new DataFrame with duplicate columns removed or renamed so that all columns are unique.\n",
    "    \"\"\"\n",
    "    # We'll build new column names for the columns we keep.\n",
    "    new_columns = [None] * len(df.columns)\n",
    "    # Keep track of which column indices to keep.\n",
    "    indices_to_keep = []\n",
    "    # Dictionary to record, for each column name, the first occurrence's Series and a count of how many we've seen.\n",
    "    seen = {}\n",
    "    \n",
    "    for i, col in enumerate(df.columns):\n",
    "        current_series = df.iloc[:, i]\n",
    "        if col not in seen:\n",
    "            # First occurrence: keep it with the original name.\n",
    "            seen[col] = {\n",
    "                'first_series': current_series,\n",
    "                'count': 1\n",
    "            }\n",
    "            new_columns[i] = col\n",
    "            indices_to_keep.append(i)\n",
    "        else:\n",
    "            # Already seen; compare with the first occurrence.\n",
    "            first_series = seen[col]['first_series']\n",
    "            if current_series.equals(first_series):\n",
    "                # Values are identical, so we want to drop this column.\n",
    "                new_columns[i] = None\n",
    "                # (Do not add its index to indices_to_keep.)\n",
    "            else:\n",
    "                # Values differ: rename this duplicate column.\n",
    "                seen[col]['count'] += 1\n",
    "                new_name = f\"{col}{rename_suffix}{seen[col]['count']}\"\n",
    "                new_columns[i] = new_name\n",
    "                indices_to_keep.append(i)\n",
    "    \n",
    "    # Build the new DataFrame using only the kept columns.\n",
    "    final_columns = [new_columns[i] for i in indices_to_keep]\n",
    "    new_df = df.iloc[:, indices_to_keep].copy()\n",
    "    new_df.columns = final_columns\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Features_build\n"
     ]
    }
   ],
   "source": [
    "print(FEAT_PATH_)\n",
    "merged_v1 = merge_parquet_files(directory=FEAT_PATH_, zero_only=True)\n",
    "merged_v3 = merge_processv3_files(directory=FEAT_PATH_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ = pd.merge(left=merged_v1, right=merged_v3, on = 'event_id')\n",
    "images_ = remove_duplicate_merge_columns(images_)\n",
    "images_ = drop_or_rename_duplicate_columns_all(images_, rename_suffix='_dup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(898, 806)\n"
     ]
    }
   ],
   "source": [
    "print(images_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train on Images Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data & Merge to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_sum(df, ):\n",
    "    # Create a new column by splitting 'event_id' at '_X_' and taking the first part\n",
    "    df['base_event_id'] = df['event_id'].str.split('_X_').str[0]\n",
    "\n",
    "    # Helper function: Count the minimum number of incidents (sorted in descending order)\n",
    "    # required to reach a cumulative precipitation sum >= 100.\n",
    "    def count_to_reach_100(x):\n",
    "        # Sort precipitation values in descending order\n",
    "        sorted_vals = x.sort_values(ascending=False)\n",
    "        # Compute the cumulative sum\n",
    "        cum_sum = sorted_vals.cumsum()\n",
    "        # Check where cumulative sum reaches or exceeds 100\n",
    "        cond = cum_sum >= 100\n",
    "        if cond.any():\n",
    "            # np.argmax returns the first index where condition is True.\n",
    "            return int(np.argmax(cond.to_numpy()) + 1)\n",
    "        else:\n",
    "            # If 100 is never reached, return the total number of incidents.\n",
    "            return len(x)\n",
    "\n",
    "    \n",
    "    # Group by the new base_event_id and aggregate the columns\n",
    "    aggregated_df = df.groupby('base_event_id', as_index=False).agg(\n",
    "        label = ('label', 'sum'),\n",
    "        precipitation_min = ('precipitation', 'min'),\n",
    "        precipitation_max = ('precipitation', 'max'),\n",
    "        precipitation_mean = ('precipitation', 'mean'),\n",
    "        precipitation_mode = ('precipitation', lambda x: x.mode().iloc[0] if not x.mode().empty else None),\n",
    "        precipitation_sum=('precipitation', 'sum'),\n",
    "        precipitation_std=('precipitation', 'std'),\n",
    "        precipitation_count_0=('precipitation', lambda x: (x == 0).sum()),\n",
    "        precipitation_count_less_5=('precipitation', lambda x: (x < 5).sum()),\n",
    "        precipitation_count_between_5_and_10=('precipitation', lambda x: ((x >= 5) & (x < 10)).sum()),\n",
    "        precipitation_count_to_reach_100=('precipitation', count_to_reach_100)\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "    return aggregated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_spictby0jfsb_X_0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_spictby0jfsb_X_1</td>\n",
       "      <td>0.095438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_spictby0jfsb_X_2</td>\n",
       "      <td>1.949560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_spictby0jfsb_X_3</td>\n",
       "      <td>3.232160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_spictby0jfsb_X_4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              event_id  precipitation  label\n",
       "0  id_spictby0jfsb_X_0       0.000000      0\n",
       "1  id_spictby0jfsb_X_1       0.095438      0\n",
       "2  id_spictby0jfsb_X_2       1.949560      0\n",
       "3  id_spictby0jfsb_X_3       3.232160      0\n",
       "4  id_spictby0jfsb_X_4       0.000000      0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ = pd.read_csv(train_file_path)\n",
    "test_ = pd.read_csv(test_file_path)\n",
    "train_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(674, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_event_id</th>\n",
       "      <th>label</th>\n",
       "      <th>precipitation_min</th>\n",
       "      <th>precipitation_max</th>\n",
       "      <th>precipitation_mean</th>\n",
       "      <th>precipitation_mode</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>precipitation_std</th>\n",
       "      <th>precipitation_count_0</th>\n",
       "      <th>precipitation_count_less_5</th>\n",
       "      <th>precipitation_count_between_5_and_10</th>\n",
       "      <th>precipitation_count_to_reach_100</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_05v6zjuaf300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.3969</td>\n",
       "      <td>2.181141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1592.232796</td>\n",
       "      <td>6.536875</td>\n",
       "      <td>581</td>\n",
       "      <td>637</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_06zma02zeea7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.9134</td>\n",
       "      <td>1.198987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>875.260653</td>\n",
       "      <td>4.075114</td>\n",
       "      <td>602</td>\n",
       "      <td>666</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_08w2po0cz63y</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.7278</td>\n",
       "      <td>0.266291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.392672</td>\n",
       "      <td>0.911627</td>\n",
       "      <td>615</td>\n",
       "      <td>726</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_092vetuky9ku</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.8956</td>\n",
       "      <td>0.276754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.030696</td>\n",
       "      <td>1.613950</td>\n",
       "      <td>668</td>\n",
       "      <td>720</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0987b1h04r48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.1455</td>\n",
       "      <td>1.882442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1374.182907</td>\n",
       "      <td>6.736390</td>\n",
       "      <td>577</td>\n",
       "      <td>649</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     base_event_id  label  precipitation_min  precipitation_max  \\\n",
       "0  id_05v6zjuaf300      1                0.0            54.3969   \n",
       "1  id_06zma02zeea7      0                0.0            51.9134   \n",
       "2  id_08w2po0cz63y      0                0.0            10.7278   \n",
       "3  id_092vetuky9ku      0                0.0            28.8956   \n",
       "4  id_0987b1h04r48      1                0.0            86.1455   \n",
       "\n",
       "   precipitation_mean  precipitation_mode  precipitation_sum  \\\n",
       "0            2.181141                 0.0        1592.232796   \n",
       "1            1.198987                 0.0         875.260653   \n",
       "2            0.266291                 0.0         194.392672   \n",
       "3            0.276754                 0.0         202.030696   \n",
       "4            1.882442                 0.0        1374.182907   \n",
       "\n",
       "   precipitation_std  precipitation_count_0  precipitation_count_less_5  \\\n",
       "0           6.536875                    581                         637   \n",
       "1           4.075114                    602                         666   \n",
       "2           0.911627                    615                         726   \n",
       "3           1.613950                    668                         720   \n",
       "4           6.736390                    577                         649   \n",
       "\n",
       "   precipitation_count_between_5_and_10  precipitation_count_to_reach_100  \\\n",
       "0                                    32                                 2   \n",
       "1                                    38                                 3   \n",
       "2                                     3                                24   \n",
       "3                                     6                                 9   \n",
       "4                                    42                                 2   \n",
       "\n",
       "    Type  \n",
       "0  Train  \n",
       "1  Train  \n",
       "2  Train  \n",
       "3  Train  \n",
       "4  Train  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_['base_event_id'] = test_['event_id'].str.split('_X_').str[0]\n",
    "test_['label'] = 0\n",
    "\n",
    "\n",
    "train = split_and_sum(train_)\n",
    "train['Type'] = 'Train'\n",
    "\n",
    "test = split_and_sum(test_)\n",
    "test['Type'] = 'Test'\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(898, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_event_id</th>\n",
       "      <th>label</th>\n",
       "      <th>precipitation_min</th>\n",
       "      <th>precipitation_max</th>\n",
       "      <th>precipitation_mean</th>\n",
       "      <th>precipitation_mode</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>precipitation_std</th>\n",
       "      <th>precipitation_count_0</th>\n",
       "      <th>precipitation_count_less_5</th>\n",
       "      <th>precipitation_count_between_5_and_10</th>\n",
       "      <th>precipitation_count_to_reach_100</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_05v6zjuaf300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.3969</td>\n",
       "      <td>2.181141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1592.232796</td>\n",
       "      <td>6.536875</td>\n",
       "      <td>581</td>\n",
       "      <td>637</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_06zma02zeea7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.9134</td>\n",
       "      <td>1.198987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>875.260653</td>\n",
       "      <td>4.075114</td>\n",
       "      <td>602</td>\n",
       "      <td>666</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_08w2po0cz63y</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.7278</td>\n",
       "      <td>0.266291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.392672</td>\n",
       "      <td>0.911627</td>\n",
       "      <td>615</td>\n",
       "      <td>726</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_092vetuky9ku</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.8956</td>\n",
       "      <td>0.276754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.030696</td>\n",
       "      <td>1.613950</td>\n",
       "      <td>668</td>\n",
       "      <td>720</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0987b1h04r48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.1455</td>\n",
       "      <td>1.882442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1374.182907</td>\n",
       "      <td>6.736390</td>\n",
       "      <td>577</td>\n",
       "      <td>649</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>id_zgckftx7lo64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.4639</td>\n",
       "      <td>1.786603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1304.220480</td>\n",
       "      <td>4.800234</td>\n",
       "      <td>566</td>\n",
       "      <td>640</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>id_zibfeb9w8fl5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6422</td>\n",
       "      <td>2.338661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1707.222526</td>\n",
       "      <td>5.973153</td>\n",
       "      <td>538</td>\n",
       "      <td>613</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>id_zk50aeed9xce</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.6604</td>\n",
       "      <td>1.363975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>995.701972</td>\n",
       "      <td>4.401754</td>\n",
       "      <td>591</td>\n",
       "      <td>665</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>id_zl678jv8h8u7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.2699</td>\n",
       "      <td>1.549485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1131.124059</td>\n",
       "      <td>5.076125</td>\n",
       "      <td>582</td>\n",
       "      <td>660</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>id_zqbp3wopp7eb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.2692</td>\n",
       "      <td>1.871746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1366.374261</td>\n",
       "      <td>6.256754</td>\n",
       "      <td>589</td>\n",
       "      <td>650</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>898 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       base_event_id  label  precipitation_min  precipitation_max  \\\n",
       "0    id_05v6zjuaf300      1                0.0            54.3969   \n",
       "1    id_06zma02zeea7      0                0.0            51.9134   \n",
       "2    id_08w2po0cz63y      0                0.0            10.7278   \n",
       "3    id_092vetuky9ku      0                0.0            28.8956   \n",
       "4    id_0987b1h04r48      1                0.0            86.1455   \n",
       "..               ...    ...                ...                ...   \n",
       "893  id_zgckftx7lo64      0                0.0            31.4639   \n",
       "894  id_zibfeb9w8fl5      0                0.0            47.6422   \n",
       "895  id_zk50aeed9xce      0                0.0            32.6604   \n",
       "896  id_zl678jv8h8u7      0                0.0            52.2699   \n",
       "897  id_zqbp3wopp7eb      0                0.0            64.2692   \n",
       "\n",
       "     precipitation_mean  precipitation_mode  precipitation_sum  \\\n",
       "0              2.181141                 0.0        1592.232796   \n",
       "1              1.198987                 0.0         875.260653   \n",
       "2              0.266291                 0.0         194.392672   \n",
       "3              0.276754                 0.0         202.030696   \n",
       "4              1.882442                 0.0        1374.182907   \n",
       "..                  ...                 ...                ...   \n",
       "893            1.786603                 0.0        1304.220480   \n",
       "894            2.338661                 0.0        1707.222526   \n",
       "895            1.363975                 0.0         995.701972   \n",
       "896            1.549485                 0.0        1131.124059   \n",
       "897            1.871746                 0.0        1366.374261   \n",
       "\n",
       "     precipitation_std  precipitation_count_0  precipitation_count_less_5  \\\n",
       "0             6.536875                    581                         637   \n",
       "1             4.075114                    602                         666   \n",
       "2             0.911627                    615                         726   \n",
       "3             1.613950                    668                         720   \n",
       "4             6.736390                    577                         649   \n",
       "..                 ...                    ...                         ...   \n",
       "893           4.800234                    566                         640   \n",
       "894           5.973153                    538                         613   \n",
       "895           4.401754                    591                         665   \n",
       "896           5.076125                    582                         660   \n",
       "897           6.256754                    589                         650   \n",
       "\n",
       "     precipitation_count_between_5_and_10  precipitation_count_to_reach_100  \\\n",
       "0                                      32                                 2   \n",
       "1                                      38                                 3   \n",
       "2                                       3                                24   \n",
       "3                                       6                                 9   \n",
       "4                                      42                                 2   \n",
       "..                                    ...                               ...   \n",
       "893                                    41                                 4   \n",
       "894                                    52                                 3   \n",
       "895                                    27                                 4   \n",
       "896                                    28                                 3   \n",
       "897                                    21                                 2   \n",
       "\n",
       "      Type  \n",
       "0    Train  \n",
       "1    Train  \n",
       "2    Train  \n",
       "3    Train  \n",
       "4    Train  \n",
       "..     ...  \n",
       "893   Test  \n",
       "894   Test  \n",
       "895   Test  \n",
       "896   Test  \n",
       "897   Test  \n",
       "\n",
       "[898 rows x 13 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.concat([train, test], ignore_index= True)\n",
    "print(full_data.shape)\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>band0_mean_efficientnet_b3.ra2_in1k_S42</th>\n",
       "      <th>band0_std_efficientnet_b3.ra2_in1k_S42</th>\n",
       "      <th>band1_mean_efficientnet_b3.ra2_in1k_S42</th>\n",
       "      <th>band1_std_efficientnet_b3.ra2_in1k_S42</th>\n",
       "      <th>band2_mean_efficientnet_b3.ra2_in1k_S42</th>\n",
       "      <th>band2_std_efficientnet_b3.ra2_in1k_S42</th>\n",
       "      <th>band3_mean_efficientnet_b3.ra2_in1k_S42</th>\n",
       "      <th>band3_std_efficientnet_b3.ra2_in1k_S42</th>\n",
       "      <th>band4_mean_efficientnet_b3.ra2_in1k_S42</th>\n",
       "      <th>...</th>\n",
       "      <th>precipitation_max</th>\n",
       "      <th>precipitation_mean</th>\n",
       "      <th>precipitation_mode</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>precipitation_std</th>\n",
       "      <th>precipitation_count_0</th>\n",
       "      <th>precipitation_count_less_5</th>\n",
       "      <th>precipitation_count_between_5_and_10</th>\n",
       "      <th>precipitation_count_to_reach_100</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_05v6zjuaf300</td>\n",
       "      <td>-0.351159</td>\n",
       "      <td>0.387995</td>\n",
       "      <td>-0.488726</td>\n",
       "      <td>0.448289</td>\n",
       "      <td>-0.457963</td>\n",
       "      <td>0.590306</td>\n",
       "      <td>1.270173</td>\n",
       "      <td>0.873391</td>\n",
       "      <td>1.149284</td>\n",
       "      <td>...</td>\n",
       "      <td>54.3969</td>\n",
       "      <td>2.181141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1592.232796</td>\n",
       "      <td>6.536875</td>\n",
       "      <td>581</td>\n",
       "      <td>637</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_066zz28m11mr</td>\n",
       "      <td>-0.267915</td>\n",
       "      <td>0.097005</td>\n",
       "      <td>-0.132496</td>\n",
       "      <td>0.174218</td>\n",
       "      <td>0.570542</td>\n",
       "      <td>0.322990</td>\n",
       "      <td>1.115806</td>\n",
       "      <td>0.377975</td>\n",
       "      <td>2.640203</td>\n",
       "      <td>...</td>\n",
       "      <td>35.8728</td>\n",
       "      <td>0.385397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>281.339884</td>\n",
       "      <td>1.884940</td>\n",
       "      <td>644</td>\n",
       "      <td>709</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_06zma02zeea7</td>\n",
       "      <td>0.126439</td>\n",
       "      <td>0.208946</td>\n",
       "      <td>0.289442</td>\n",
       "      <td>0.312349</td>\n",
       "      <td>1.065926</td>\n",
       "      <td>0.582471</td>\n",
       "      <td>2.389661</td>\n",
       "      <td>0.619813</td>\n",
       "      <td>5.054065</td>\n",
       "      <td>...</td>\n",
       "      <td>51.9134</td>\n",
       "      <td>1.198987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>875.260653</td>\n",
       "      <td>4.075114</td>\n",
       "      <td>602</td>\n",
       "      <td>666</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_073l04ir88sn</td>\n",
       "      <td>0.969935</td>\n",
       "      <td>0.433456</td>\n",
       "      <td>0.774585</td>\n",
       "      <td>0.467205</td>\n",
       "      <td>1.052091</td>\n",
       "      <td>0.595688</td>\n",
       "      <td>1.732535</td>\n",
       "      <td>0.795263</td>\n",
       "      <td>2.823037</td>\n",
       "      <td>...</td>\n",
       "      <td>162.3970</td>\n",
       "      <td>2.116429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1544.993471</td>\n",
       "      <td>9.632064</td>\n",
       "      <td>567</td>\n",
       "      <td>654</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_08w2po0cz63y</td>\n",
       "      <td>-0.058516</td>\n",
       "      <td>0.095812</td>\n",
       "      <td>-0.026293</td>\n",
       "      <td>0.144664</td>\n",
       "      <td>0.402103</td>\n",
       "      <td>0.194872</td>\n",
       "      <td>0.659779</td>\n",
       "      <td>0.212870</td>\n",
       "      <td>1.796405</td>\n",
       "      <td>...</td>\n",
       "      <td>10.7278</td>\n",
       "      <td>0.266291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.392672</td>\n",
       "      <td>0.911627</td>\n",
       "      <td>615</td>\n",
       "      <td>726</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 819 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          event_id  band0_mean_efficientnet_b3.ra2_in1k_S42  \\\n",
       "0  id_05v6zjuaf300                                -0.351159   \n",
       "1  id_066zz28m11mr                                -0.267915   \n",
       "2  id_06zma02zeea7                                 0.126439   \n",
       "3  id_073l04ir88sn                                 0.969935   \n",
       "4  id_08w2po0cz63y                                -0.058516   \n",
       "\n",
       "   band0_std_efficientnet_b3.ra2_in1k_S42  \\\n",
       "0                                0.387995   \n",
       "1                                0.097005   \n",
       "2                                0.208946   \n",
       "3                                0.433456   \n",
       "4                                0.095812   \n",
       "\n",
       "   band1_mean_efficientnet_b3.ra2_in1k_S42  \\\n",
       "0                                -0.488726   \n",
       "1                                -0.132496   \n",
       "2                                 0.289442   \n",
       "3                                 0.774585   \n",
       "4                                -0.026293   \n",
       "\n",
       "   band1_std_efficientnet_b3.ra2_in1k_S42  \\\n",
       "0                                0.448289   \n",
       "1                                0.174218   \n",
       "2                                0.312349   \n",
       "3                                0.467205   \n",
       "4                                0.144664   \n",
       "\n",
       "   band2_mean_efficientnet_b3.ra2_in1k_S42  \\\n",
       "0                                -0.457963   \n",
       "1                                 0.570542   \n",
       "2                                 1.065926   \n",
       "3                                 1.052091   \n",
       "4                                 0.402103   \n",
       "\n",
       "   band2_std_efficientnet_b3.ra2_in1k_S42  \\\n",
       "0                                0.590306   \n",
       "1                                0.322990   \n",
       "2                                0.582471   \n",
       "3                                0.595688   \n",
       "4                                0.194872   \n",
       "\n",
       "   band3_mean_efficientnet_b3.ra2_in1k_S42  \\\n",
       "0                                 1.270173   \n",
       "1                                 1.115806   \n",
       "2                                 2.389661   \n",
       "3                                 1.732535   \n",
       "4                                 0.659779   \n",
       "\n",
       "   band3_std_efficientnet_b3.ra2_in1k_S42  \\\n",
       "0                                0.873391   \n",
       "1                                0.377975   \n",
       "2                                0.619813   \n",
       "3                                0.795263   \n",
       "4                                0.212870   \n",
       "\n",
       "   band4_mean_efficientnet_b3.ra2_in1k_S42  ...  precipitation_max  \\\n",
       "0                                 1.149284  ...            54.3969   \n",
       "1                                 2.640203  ...            35.8728   \n",
       "2                                 5.054065  ...            51.9134   \n",
       "3                                 2.823037  ...           162.3970   \n",
       "4                                 1.796405  ...            10.7278   \n",
       "\n",
       "   precipitation_mean  precipitation_mode  precipitation_sum  \\\n",
       "0            2.181141                 0.0        1592.232796   \n",
       "1            0.385397                 0.0         281.339884   \n",
       "2            1.198987                 0.0         875.260653   \n",
       "3            2.116429                 0.0        1544.993471   \n",
       "4            0.266291                 0.0         194.392672   \n",
       "\n",
       "   precipitation_std  precipitation_count_0  precipitation_count_less_5  \\\n",
       "0           6.536875                    581                         637   \n",
       "1           1.884940                    644                         709   \n",
       "2           4.075114                    602                         666   \n",
       "3           9.632064                    567                         654   \n",
       "4           0.911627                    615                         726   \n",
       "\n",
       "   precipitation_count_between_5_and_10  precipitation_count_to_reach_100  \\\n",
       "0                                    32                                 2   \n",
       "1                                    19                                 8   \n",
       "2                                    38                                 3   \n",
       "3                                    36                                 1   \n",
       "4                                     3                                24   \n",
       "\n",
       "    Type  \n",
       "0  Train  \n",
       "1   Test  \n",
       "2  Train  \n",
       "3   Test  \n",
       "4  Train  \n",
       "\n",
       "[5 rows x 819 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(left = images_, right = full_data, left_on = 'event_id', right_on = 'base_event_id', how = 'left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Images Training - Separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN - R4 Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_0 =  {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting': 'gbdt',\n",
    "        'n_estimators': 5_000,\n",
    "        'learning_rate': 0.025, \n",
    "        'feature_fraction': 0.85, \n",
    "        'bagging_fraction': 0.92, \n",
    "        'bagging_freq': 40, \n",
    "        'num_leaves': 99,\n",
    "        'verbose': -1,\n",
    "        'seed': 42,\n",
    "    }\n",
    "\n",
    "params_1 =  {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting': 'gbdt',\n",
    "    'n_estimators': 5_000,\n",
    "    'learning_rate': 0.00820777905418306, \n",
    "    'feature_fraction': 0.8017433735476059, \n",
    "    'bagging_fraction': 0.9379166782699045, \n",
    "    'bagging_freq': 43, \n",
    "    'num_leaves': 95,\n",
    "    'verbose': -1,\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "params_2 =  {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting': 'gbdt',\n",
    "    'n_estimators': 5_000,\n",
    "    'learning_rate': 0.033893435894644865, \n",
    "    'feature_fraction': 0.857238873524995, \n",
    "    'bagging_fraction': 0.7055416618687687, \n",
    "    'bagging_freq': 32, \n",
    "    'num_leaves': 30,\n",
    "    'verbose': -1,\n",
    "    'seed': 42,\n",
    "}\n",
    "params_3 =   {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting': 'gbdt',\n",
    "    \n",
    "\n",
    "    'learning_rate': 0.02872672034985086, \n",
    "    'feature_fraction': 0.8573811600751811, \n",
    "    'bagging_fraction': 0.7051969782608369, \n",
    "    'bagging_freq': 23, \n",
    "    'num_leaves': 62, \n",
    "    'n_estimators': 5796, \n",
    "    'max_depth': 45,        \n",
    "\n",
    "    'verbose': -1,\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm_with_cv(df, params):\n",
    "    # --- 1. Split the full DataFrame into train and test using the 'Type' column ---\n",
    "    # Assuming 'Type' column has values like 'train' and 'test'\n",
    "    train_df = df[df['Type'] == 'Train'].copy()\n",
    "    test_df = df[df['Type'] == 'Test'].copy()\n",
    "    \n",
    "    # Save event_id for train and test to attach later in OOF and test predictions\n",
    "    train_ids = train_df[['event_id']].copy()\n",
    "    test_ids  = test_df[['event_id']].copy()\n",
    "    \n",
    "    # Drop columns not used as features in training\n",
    "    # We drop 'event_id', 'base_event_id', and 'Type'\n",
    "    drop_cols = ['event_id', 'base_event_id', 'Type']\n",
    "    train_df = train_df.drop(columns=drop_cols)\n",
    "    test_df  = test_df.drop(columns=drop_cols)\n",
    "    \n",
    "    # Separate features and target for train data\n",
    "    X = train_df.drop(columns=['label'])\n",
    "    y = train_df['label']\n",
    "    \n",
    "    # For test, if label is present, drop it (typically test is unlabeled)\n",
    "    X_test = test_df.drop(columns=['label'], errors='ignore')\n",
    "    \n",
    "    # Show label distribution to verify stratification will work\n",
    "    print(\"Label distribution in train set:\\n\", y.value_counts())\n",
    "    \n",
    "    # Create 5 stratified folds on the train set\n",
    "    skf = StratifiedKFold(n_splits=15, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize arrays for out-of-fold predictions and test predictions\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "    \n",
    "    \n",
    "    # 5-Fold Cross Validation\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"--- Fold {fold+1} ---\")\n",
    "        # Ensure we have non-empty splits\n",
    "        if len(train_idx) == 0 or len(valid_idx) == 0:\n",
    "            print(f\"Skipping fold {fold+1} due to insufficient data.\")\n",
    "            continue\n",
    "        \n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        # Prepare LightGBM datasets\n",
    "        lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
    "\n",
    "        \n",
    "        # Train the model with early stopping\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            num_boost_round=3000,\n",
    "            valid_sets=[lgb_train, lgb_valid],\n",
    "            valid_names=['train', 'valid'],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=250, verbose=False),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Predict on validation fold\n",
    "        oof_preds[valid_idx] = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "        \n",
    "        # Predict on test set and average over folds\n",
    "        test_preds += model.predict(X_test, num_iteration=model.best_iteration) / skf.n_splits\n",
    "\n",
    "    # Evaluate model performance on training data using OOF predictions\n",
    "    oof_logloss = log_loss(y, oof_preds)\n",
    "    print(\"\\nOOF Binary Log Loss:\", oof_logloss)\n",
    "    \n",
    "    # Convert probabilities to binary predictions (using threshold 0.5)\n",
    "    oof_binary = (oof_preds >= 0.5).astype(int)\n",
    "    conf_mat = confusion_matrix(y, oof_binary)\n",
    "    print(\"\\nConfusion Matrix (OOF predictions):\")\n",
    "    print(conf_mat)\n",
    "    \n",
    "    # Create an OOF DataFrame that includes the original event_id, true label, and predictions\n",
    "    oof_df = train_ids.copy()\n",
    "    oof_df['label'] = y.values\n",
    "    oof_df['oof_pred'] = oof_preds\n",
    "    oof_df['predicted_label'] = oof_binary\n",
    "\n",
    "    test_ids['preds'] =  test_preds\n",
    "    test_ids['predicted_label'] = (test_preds >= 0.5).astype(int)\n",
    "\n",
    "    \n",
    "    return oof_df, test_preds, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in train set:\n",
      " label\n",
      "0    356\n",
      "1    318\n",
      "Name: count, dtype: int64\n",
      "--- Fold 1 ---\n",
      "--- Fold 2 ---\n",
      "--- Fold 3 ---\n",
      "--- Fold 4 ---\n",
      "--- Fold 5 ---\n",
      "--- Fold 6 ---\n",
      "--- Fold 7 ---\n",
      "--- Fold 8 ---\n",
      "--- Fold 9 ---\n",
      "--- Fold 10 ---\n",
      "--- Fold 11 ---\n",
      "--- Fold 12 ---\n",
      "--- Fold 13 ---\n",
      "--- Fold 14 ---\n",
      "--- Fold 15 ---\n",
      "\n",
      "OOF Binary Log Loss: 0.23559340295319595\n",
      "\n",
      "Confusion Matrix (OOF predictions):\n",
      "[[329  27]\n",
      " [ 38 280]]\n"
     ]
    }
   ],
   "source": [
    "oof_preds, test_preds, test_ids = train_lgbm_with_cv(df, params_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids.to_csv(f'IDSx_TEST_FromImages_S{SEED_}_PR1.csv',index=False)\n",
    "oof_preds.to_csv(f'IDSx_OOF_FromImages_S{SEED_}_PR1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "==> 0\n",
    "OOF Binary Log Loss: 0.23790857289877537 - x67\n",
    "Confusion Matrix (OOF predictions):\n",
    "[[330  26]\n",
    "[ 41 277]]\n",
    "\n",
    "\n",
    "==> 1\n",
    "OOF Binary Log Loss: 0.23559340295319595 - x65\n",
    "Confusion Matrix (OOF predictions):\n",
    "[[329  27]\n",
    "[ 38 280]]\n",
    "\n",
    "\n",
    "==> 2\n",
    "OOF Binary Log Loss: 0.23777142855402644 - x65\n",
    "Confusion Matrix (OOF predictions):\n",
    "[[332  24]\n",
    "[ 41 277]]\n",
    "\n",
    "\n",
    "==> 3\n",
    "OOF Binary Log Loss: 0.23596330482794922 - x65\n",
    "Confusion Matrix (OOF predictions):\n",
    "[[329  27]\n",
    "[ 38 280]]\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The END of Process & Usage of Images ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CSV Files & LAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Start of CSV Files - LAGS ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_file_path)\n",
    "data_test = pd.read_csv(test_file_path)\n",
    "\n",
    "\n",
    "data['event_id'] = data['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "data['event_idx'] = data.groupby('event_id', sort=False).ngroup()\n",
    "data_test['event_id'] = data_test['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "data_test['event_idx'] = data_test.groupby('event_id', sort=False).ngroup()\n",
    "\n",
    "data['event_t'] = data.groupby('event_id').cumcount()\n",
    "data_test['event_t'] = data_test.groupby('event_id').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_precipitation_lag_mean(df, lag_range=(1, 365)):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame by grouping by 'event_id' and creating lag features for the 'precipitation' column.\n",
    "    Then, it calculates the mean of these lagged values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing at least 'event_id' and 'precipitation' columns.\n",
    "        lag_range (tuple): A tuple specifying the range of lags to create (start, end).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with calculated lag mean columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check required columns exist\n",
    "    if 'event_id' not in df.columns or 'precipitation' not in df.columns:\n",
    "        raise ValueError(\"The DataFrame must contain 'event_id' and 'precipitation' columns.\")\n",
    "\n",
    "    # Extract the start and end of the lag range\n",
    "    lag_start, lag_end = lag_range\n",
    "\n",
    "    # Group by 'event_id' and process each group\n",
    "    result = []\n",
    "    for event_id, group in tqdm(df.groupby('event_id')):\n",
    "        group = group.sort_index()  # Sort by index (assuming time series data)\n",
    "\n",
    "        # Create lag features for the specified range\n",
    "        for lag in range(lag_start, lag_end + 1):\n",
    "            group[f'precipitation_lag_{lag}'] = group['precipitation'].shift(lag)\n",
    "\n",
    "        # Calculate the mean of the lagged values\n",
    "        lag_columns = [f'precipitation_lag_{lag}' for lag in range(lag_start, lag_end + 1)]\n",
    "        group['precipitation_lag_mean'] = group[lag_columns].mean(axis=1)\n",
    "\n",
    "        result.append(group)\n",
    "\n",
    "    # Concatenate all groups back into a single DataFrame\n",
    "    result_df = pd.concat(result)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 674/674 [03:18<00:00,  3.40it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [01:06<00:00,  3.34it/s]\n"
     ]
    }
   ],
   "source": [
    "prefix = 'V9'\n",
    "data_          = process_precipitation_lag_mean(data, lag_range=(-365, 730))\n",
    "data_test_     = process_precipitation_lag_mean(data_test, lag_range=(-365, 730))\n",
    "data_.to_parquet(f\"{work_dir}/Simple_Train{prefix}_RAW.parquet\")\n",
    "data_test_.to_parquet(f\"{work_dir}/Simple_Test{prefix}_RAW.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2  -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_version = 2\n",
    "\n",
    "enable_tabular_predictions = True\n",
    "tabular_targeted = '_S42_PR1'\n",
    "\n",
    "\n",
    "data_dir        = 'data'\n",
    "\n",
    "\n",
    "train_path_     =  \"./data/Train.csv\"\n",
    "test_path_      =  './data/Test.csv'\n",
    "\n",
    "SEED_ = 42\n",
    "\n",
    "#image_feat_eng_path = \"./data/Satellite_Features_ENG.csv\"\n",
    "#table_expanded_path = \"./data/Expanded_PRECP_Features.csv\"\n",
    "\n",
    "#building_block_mode = 'single'\n",
    "#building_block_path  = \"./data/building_block_256.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sin_cos_cycles(df, column_name, ver_ = 2):\n",
    "    \"\"\"\n",
    "    Calculate sine and cosine values for different cycles (24-hour, 7-day, 30-day, 365-day, and normalized) \n",
    "    for a given column in a DataFrame.\n",
    "    \n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in the DataFrame\")\n",
    "    \n",
    "    # Calculate sine and cosine for different cycles\n",
    "    df['sin_24hr'] = np.sin(2 * np.pi * df[column_name] / 24)\n",
    "    df['cos_24hr'] = np.cos(2 * np.pi * df[column_name] / 24)\n",
    "\n",
    "    df['sin_7day'] = np.sin(2 * np.pi * df[column_name] / (7 * 24))\n",
    "    df['cos_7day'] = np.cos(2 * np.pi * df[column_name] / (7 * 24))\n",
    "\n",
    "    df['sin_14day'] = np.sin(2 * np.pi * df[column_name] / (14 * 24))\n",
    "    df['cos_14day'] = np.cos(2 * np.pi * df[column_name] / (14 * 24))\n",
    "\n",
    "    df['sin_21day'] = np.sin(2 * np.pi * df[column_name] / (21 * 24))\n",
    "    df['cos_21day'] = np.cos(2 * np.pi * df[column_name] / (21 * 24))\n",
    "\n",
    "    df['sin_30day'] = np.sin(2 * np.pi * df[column_name] / (30 * 24))\n",
    "    df['cos_30day'] = np.cos(2 * np.pi * df[column_name] / (30 * 24))\n",
    "\n",
    "    df['sin_60day'] = np.sin(2 * np.pi * df[column_name] / (60 * 24))\n",
    "    df['cos_60day'] = np.cos(2 * np.pi * df[column_name] / (60 * 24))\n",
    "\n",
    "    df['sin_365day'] = np.sin(2 * np.pi * df[column_name] / (365 * 24))\n",
    "    df['cos_365day'] = np.cos(2 * np.pi * df[column_name] / (365 * 24))\n",
    "\n",
    "    if ver_ in [2]:\n",
    "\n",
    "        df['sin_2day'] = np.sin(2 * np.pi * df[column_name] / (2 * 24))\n",
    "        df['cos_2day'] = np.cos(2 * np.pi * df[column_name] / (2 * 24))\n",
    "\n",
    "        df['sin_3day'] = np.sin(2 * np.pi * df[column_name] / (3 * 24))\n",
    "        df['cos_3day'] = np.cos(2 * np.pi * df[column_name] / (3 * 24))\n",
    "\n",
    "        df['sin_4day'] = np.sin(2 * np.pi * df[column_name] / (4 * 24))\n",
    "        df['cos_4day'] = np.cos(2 * np.pi * df[column_name] / (4 * 24))\n",
    "\n",
    "        df['sin_5day'] = np.sin(2 * np.pi * df[column_name] / (5 * 24))\n",
    "        df['cos_5day'] = np.cos(2 * np.pi * df[column_name] / (5 * 24))\n",
    "\n",
    "\n",
    "        df['sin_90day'] = np.sin(2 * np.pi * df[column_name] / (90 * 24))\n",
    "        df['cos_90day'] = np.cos(2 * np.pi * df[column_name] / (90 * 24))\n",
    "\n",
    "        df['sin_120day'] = np.sin(2 * np.pi * df[column_name] / (120 * 24))\n",
    "        df['cos_120day'] = np.cos(2 * np.pi * df[column_name] / (120 * 24))\n",
    "\n",
    "        df['sin_180day'] = np.sin(2 * np.pi * df[column_name] / (180 * 24))\n",
    "        df['cos_180day'] = np.cos(2 * np.pi * df[column_name] / (180 * 24))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    # Normalized cycle (assuming input is normalized to 1)\n",
    "    df['sin_normal'] = np.sin(2 * np.pi * df[column_name])\n",
    "    df['cos_normal'] = np.cos(2 * np.pi * df[column_name])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_path_)\n",
    "data_test = pd.read_csv(test_path_)\n",
    "\n",
    "\n",
    "data['event_id'] = data['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "data['event_idx'] = data.groupby('event_id', sort=False).ngroup()\n",
    "data_test['event_id'] = data_test['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
    "data_test['event_idx'] = data_test.groupby('event_id', sort=False).ngroup()\n",
    "\n",
    "data['event_t'] = data.groupby('event_id').cumcount()\n",
    "data_test['event_t'] = data_test.groupby('event_id').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'V9'\n",
    "data = pd.read_parquet(os.path.join(data_dir, f\"Simple_Train{prefix}_RAW.parquet\"))\n",
    "data_test = pd.read_parquet(os.path.join(data_dir,f\"Simple_Test{prefix}_RAW.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = calculate_sin_cos_cycles(data, 'event_t', ver_ = sin_version)\n",
    "data_test = calculate_sin_cos_cycles(data_test, 'event_t', ver_ = sin_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_tabular_predictions:\n",
    "\n",
    "    tr_ = pd.read_csv(f'IDSx_OOF_FromImages{tabular_targeted}.csv').drop(columns=['label']).rename(columns={'oof_pred': 'model_pred'})\n",
    "    te_ = pd.read_csv(f'IDSx_TEST_FromImages{tabular_targeted}.csv').rename(columns={'preds': 'model_pred'})\n",
    "\n",
    "    # tr_ = rw_[rw_['Type'] == 'Train'].drop(columns=['Type'])\n",
    "    # te_ = rw_[rw_['Type'] == 'Test'].drop(columns=['Type'])\n",
    "\n",
    "    display(tr_.head())\n",
    "    display(te_.head())\n",
    "\n",
    "    print(data.shape)\n",
    "    data = pd.merge(left=data, right = tr_, on ='event_id', how='left')\n",
    "    print(data.shape)\n",
    "\n",
    "    print(data_test.shape)\n",
    "    data_test = pd.merge(left=data_test, right = te_, on ='event_id', how='left')\n",
    "    print(data_test.shape)\n",
    "    data_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features and target\n",
    "X = data.drop(columns=['label', 'event_id', 'event_idx', 'event_t'])\n",
    "y = data['label']\n",
    "groups = data['event_id']\n",
    "\n",
    "\n",
    "skf = StratifiedGroupKFold(n_splits=10, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_ = { 'max_depth': 10, \n",
    "                'num_leaves': 297, \n",
    "                'learning_rate': 0.012899582615551406, \n",
    "                'feature_fraction': 0.8543108407301498, \n",
    "                'bagging_fraction': 0.882130759492644, \n",
    "                'bagging_freq': 80, \n",
    "                'min_data_in_leaf': 6752, \n",
    "                'lambda_l1': 0.11023697527638228, \n",
    "                'lambda_l2': 3.494274960171233e-05, \n",
    "                'n_estimators': 11000, \n",
    "                'min_gain_to_split': 0.3815117888101758, \n",
    "                'scale_pos_weight': 3.1728213698029695, \n",
    "                'is_unbalanced': True,}\n",
    "\n",
    "\n",
    "best_other_ = { 'init_score': 0.01}\n",
    "\n",
    "\n",
    "lgb_params = {\n",
    "            'objective': 'binary',  \n",
    "            'metric': 'binary_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            \n",
    "            ## Best parameters from optuna\n",
    "            'max_depth': best_model_['max_depth'],\n",
    "            'num_leaves': best_model_['num_leaves'],\n",
    "            'learning_rate': best_model_['learning_rate'],\n",
    "            'feature_fraction': best_model_['feature_fraction'],\n",
    "            'bagging_fraction': best_model_['bagging_fraction'],\n",
    "            'bagging_freq': best_model_['bagging_freq'],\n",
    "            'min_data_in_leaf': best_model_['min_data_in_leaf'],\n",
    "            'lambda_l1': best_model_['lambda_l1'],\n",
    "            'lambda_l2': best_model_['lambda_l2'],\n",
    "            'n_estimators' : best_model_['n_estimators'],\n",
    "            'min_gain_to_split': best_model_['min_gain_to_split'],\n",
    "\n",
    "            \n",
    "            'scale_pos_weight': best_model_['scale_pos_weight'],\n",
    "            'is_unbalanced' : best_model_['is_unbalanced'],\n",
    "            \n",
    "            # Additional parameters for new LightGBM versions:\n",
    "            'two_round': True,\n",
    "            'num_threads': 64,\n",
    "            'force_col_wise': True,\n",
    "            'verbose': -1,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_ = '29.2_SIN2_S24_R1_'\n",
    "log_filename = 'local_log.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the best models, logloss, and OOF predictions\n",
    "lgb_models = []\n",
    "lgb_logloss = []\n",
    "oof_preds = np.zeros(len(X))  # Placeholder for OOF predictions\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "# Cross-validation\n",
    "for train_idx, valid_idx in skf.split(X, y, groups):\n",
    "    print(f'\\n Start training for fold: {cnt}')\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "\n",
    "    # Prepare the message including the current timestamp\n",
    "    current_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "    message = f'\\n\\n\\nV{PREFIX_} - No Images -{current_time} - Start training for fold: {cnt}\\n'\n",
    "    # Append the message to the log file\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(message)\n",
    "\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    # train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    # valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "    train_init_score = np.full(X_train.shape[0], best_other_['init_score'])\n",
    "    valid_init_score = np.full(X_valid.shape[0], best_other_['init_score'])\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, init_score=train_init_score)\n",
    "    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data, init_score=valid_init_score)\n",
    "    print(\"Custom Init_Score\")\n",
    "\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round = 3000,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=1_500, verbose=True),\n",
    "            ]\n",
    "        #early_stopping_rounds=500,  # Early stopping of 500\n",
    "        #verbose_eval=False\n",
    "    )\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    elapsed_time_minutes = (end_time - start_time) / 60  # Calculate elapsed time in minutes\n",
    "    \n",
    "    time_message = f\"Total time for fold {cnt}: {elapsed_time_minutes:.2f} minutes\"\n",
    "    print(time_message)\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(time_message)\n",
    "\n",
    "    # OOF predictions for the validation fold\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    oof_preds[valid_idx] = y_valid_pred  # Store OOF predictions\n",
    "    logloss = log_loss(y_valid, y_valid_pred)\n",
    "    print(f\"\\nLogloss for LightGBM for fold: {cnt} is {logloss}\")\n",
    "\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(f\"\\nLogloss for LightGBM for fold: {cnt} is {logloss}\")\n",
    "    lgb_logloss.append(logloss)\n",
    "\n",
    "    lgb_models.append(model)\n",
    "    cnt += 1\n",
    "\n",
    "print(f\"Mean Logloss for LightGBM: {np.mean(lgb_logloss)}\")\n",
    "\n",
    "with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(f\"\\nMean Logloss for LightGBM: {np.mean(lgb_logloss)} \\n\\n---------------------------------------------------- \\n\\n\")\n",
    "\n",
    "# Create a DataFrame for OOF predictions with `event_id`\n",
    "oof_df = data[['event_id','event_idx','event_t']].copy()\n",
    "oof_df['oof_pred'] = oof_preds\n",
    "\n",
    "# Save the OOF predictions to a CSV file\n",
    "oof_df.to_csv(f'{PREFIX_}oof_predictions.csv', index=False)\n",
    "print(\"OOF predictions saved to 'oof_predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test dataset using an ensemble of models\n",
    "def predict_lgb(models, data_test):\n",
    "    predictions = np.zeros(len(data_test))\n",
    "    for model in models:\n",
    "        predictions += model.predict(data_test)\n",
    "    return predictions / len(models)\n",
    "\n",
    "# Predict on the test set (replace 'data_test' with actual test data)\n",
    "data_test_predictions_lgb = predict_lgb(lgb_models, data_test.drop(columns=['event_id', 'event_idx', 'event_t']))\n",
    "data_test['label_lgbm'] = data_test_predictions_lgb\n",
    "\n",
    "\n",
    "data_test['event_id'] = data_test['event_id']+'_X_'+data_test['event_t'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save File of inference\n",
    "data_test[['event_id', 'label_lgbm']].to_csv(f\"V{PREFIX_}_submission_S{SEED_}_tbl_Heavy.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
