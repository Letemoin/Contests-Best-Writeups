{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7efa4ef",
   "metadata": {},
   "source": [
    "# Roof-Counting A2Z Pipeline\n",
    "\n",
    "This notebook runs **all five** training experiments (PPYOLOE X Fast + four RTMDet variants) and then performs inference + **Weighted-Boxes-Fusion** ensembling across their outputs.  \n",
    "\n",
    "**Sections**  \n",
    "1. Setup & Dependencies  \n",
    "2. Imports  \n",
    "3. Configuration (paths & hyperparams)  \n",
    "4. Data Preparation (CSV → COCO + train/val split)  \n",
    "5. Experiment List (six configs)  \n",
    "6. Training Loop  \n",
    "7. Inference & Single-Model JSON Exports  \n",
    "8. Ensembling & Submission  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab75be",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mmcv-full mmdet mmyolo openmim ensemble-boxes pycocotools sklearn wandb\n",
    "!mim install mmengine mmcv mmdet mmyolo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a15bb",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "import mmcv\n",
    "from mmcv import Config\n",
    "from mmdet.apis import set_random_seed, train_detector, init_detector, inference_detector\n",
    "from ensemble_boxes import weighted_boxes_fusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109110ee",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8912a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"HOME\": \"/notebooks/ZINDI\",\n",
    "    \"CSV_TRAIN\": \"/notebooks/ZINDI/Train.csv\",\n",
    "    \"CSV_TEST\":  \"/notebooks/ZINDI/Test.csv\",\n",
    "    \"SAMPLE_SUB\": \"/notebooks/ZINDI/SampleSubmission.csv\",\n",
    "    \"IMAGE_FOLDER\": \"/notebooks/ZINDI/Images\",\n",
    "    \"WORK_DIR\": \"work_dirs\",\n",
    "    \"COCO_DIR\": \"coco\",\n",
    "    \"VAL_SPLIT\": 0.1,\n",
    "    # Inference thresholds\n",
    "    \"BOX_SMALL_FACTOR\": 0.02,\n",
    "    \"ENSEMBLE_IOU_THR\": 0.5,\n",
    "    \"SCORE_THR\": 0.4\n",
    "}\n",
    "os.makedirs(CFG[\"WORK_DIR\"], exist_ok=True)\n",
    "os.makedirs(CFG[\"COCO_DIR\"], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be086120",
   "metadata": {},
   "source": [
    "## 4. Data Preparation: CSV → COCO & Stratified Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd502aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_dimensions(path):\n",
    "    with Image.open(path) as img:\n",
    "        return img.width, img.height\n",
    "\n",
    "def csv_to_coco(csv_file, image_folder):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Expect columns: ImageId, XMin, YMin, XMax, YMax, ClassId\n",
    "    cats = [{\"id\":1,\"name\":\"Other\"},{\"id\":2,\"name\":\"Tin\"},{\"id\":3,\"name\":\"Thatch\"}]\n",
    "    img_ids = list(df[\"ImageId\"].unique())\n",
    "    images, annotations = [], []\n",
    "    id_map = {img: idx+1 for idx, img in enumerate(img_ids)}\n",
    "    ann_id = 1\n",
    "    for img in img_ids:\n",
    "        w,h = get_image_dimensions(f\"{image_folder}/{img}.tif\")\n",
    "        images.append({\"id\": id_map[img], \"file_name\": f\"{img}.tif\", \"width\":w, \"height\":h})\n",
    "    for _, row in df.iterrows():\n",
    "        x, y, x2, y2 = row[\"XMin\"], row[\"YMin\"], row[\"XMax\"], row[\"YMax\"]\n",
    "        w, h = x2-x, y2-y\n",
    "        annotations.append({\n",
    "            \"id\": ann_id,\n",
    "            \"image_id\": id_map[row[\"ImageId\"]],\n",
    "            \"category_id\": int(row[\"ClassId\"]),\n",
    "            \"bbox\": [x, y, w, h],\n",
    "            \"area\": w*h,\n",
    "            \"iscrowd\": 0,\n",
    "            \"segmentation\": [[x, y, x+w, y, x+w, y+h, x, y+h]]\n",
    "        })\n",
    "        ann_id += 1\n",
    "    return {\"images\":images, \"annotations\":annotations, \"categories\":cats}\n",
    "\n",
    "def stratified_split(coco, val_size=0.1, seed=42):\n",
    "    pairs = [(anno[\"image_id\"], anno[\"category_id\"]) for anno in coco[\"annotations\"]]\n",
    "    train_ids, val_ids = train_test_split(pairs, test_size=val_size, random_state=seed, stratify=[c for _,c in pairs])\n",
    "    train_img_ids = {i for i,_ in train_ids}\n",
    "    val_img_ids   = {i for i,_ in val_ids}\n",
    "    def filter_ids(ids):\n",
    "        imgs = [img for img in coco[\"images\"] if img[\"id\"] in ids]\n",
    "        ann  = [anno for anno in coco[\"annotations\"] if anno[\"image_id\"] in ids]\n",
    "        return {\"images\":imgs, \"annotations\":ann, \"categories\":coco[\"categories\"]}\n",
    "    return filter_ids(train_img_ids), filter_ids(val_img_ids)\n",
    "\n",
    "# Build & write COCO JSONs\n",
    "coco_all = csv_to_coco(CFG[\"CSV_TRAIN\"], CFG[\"IMAGE_FOLDER\"])\n",
    "train_coco, val_coco = stratified_split(coco_all, CFG[\"VAL_SPLIT\"])\n",
    "with open(f'{CFG[\"COCO_DIR\"]}/instances_train.json','w') as f: json.dump(train_coco, f)\n",
    "with open(f'{CFG[\"COCO_DIR\"]}/instances_val.json','w')   as f: json.dump(val_coco,   f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e27b1",
   "metadata": {},
   "source": [
    "## 5. Experiment List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76900c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define each of the six runs (five training )\n",
    "EXPERIMENTS = [\n",
    "    # YOLO\n",
    "    {\n",
    "      \"name\":\"ppyoloe_x_fast\",\n",
    "      \"mmdet_cfg\":\"mmyolo/configs/ppyoloe/ppyoloe_x_fast_8xb16-300e_coco.py\",\n",
    "      \"batch\":16, \"max_epochs\":300, \"resume\":False, \"wandb\":False, \"lr\":1e-3\n",
    "    },\n",
    "    # RTMDet L variants\n",
    "    {\"name\":\"rtmdet_l_fast\",\"mmdet_cfg\":\"configs/rtmdet/rtmdet_l_syncbn_fast_8xb8-100e_coco.py\",\"batch\":8,\"max_epochs\":100,\"resume\":True,\"wandb\":True,\"lr\":4e-3},\n",
    "    {\"name\":\"rtmdet_l_lp\",\"mmdet_cfg\":\"configs/rtmdet/rtmdet_l_syncbn_fast_8xb2-400e_coco.py\",\"batch\":2,\"max_epochs\":400,\"resume\":True,\"wandb\":False,\"lr\":1e-4},\n",
    "    {\"name\":\"rtmdet_x_fast\",\"mmdet_cfg\":\"configs/rtmdet/rtmdet_x_syncbn_fast_8xb8-200e_coco.py\",\"batch\":8,\"max_epochs\":200,\"resume\":False,\"wandb\":True,\"lr\":1e-3},\n",
    "    {\"name\":\"rtmdet_l_mid\",\"mmdet_cfg\":\"configs/rtmdet/rtmdet_l_syncbn_fast_8xb4-100e_coco.py\",\"batch\":4,\"max_epochs\":100,\"resume\":True,\"wandb\":False,\"lr\":4e-3}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee51ca",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071228f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "def train_experiment(exp):\n",
    "    # load base config\n",
    "    cfg = Config.fromfile(exp[\"mmdet_cfg\"])\n",
    "    # overrides\n",
    "    cfg.data.train.ann_file = f\"{CFG['COCO_DIR']}/instances_train.json\"\n",
    "    cfg.data.train.img_prefix = CFG[\"IMAGE_FOLDER\"]\n",
    "    cfg.data.val.ann_file   = f\"{CFG['COCO_DIR']}/instances_val.json\"\n",
    "    cfg.data.val.img_prefix = CFG[\"IMAGE_FOLDER\"]\n",
    "    cfg.optimizer.lr = exp[\"lr\"]\n",
    "    cfg.lr_config by_epoch = True\n",
    "    cfg.runner.max_epochs = exp[\"max_epochs\"]\n",
    "    cfg.data.samples_per_gpu = exp[\"batch\"]\n",
    "    cfg.work_dir = os.path.join(CFG[\"WORK_DIR\"], exp[\"name\"])\n",
    "    cfg.load_from = cfg.load_from if exp[\"resume\"] else None\n",
    "    # wandb hook\n",
    "    if exp[\"wandb\"]:\n",
    "        cfg.log_config.hooks.append({\"type\":\"WandbLoggerHook\", \"init_kwargs\":{\"project\":\"zindi-roof\"}})\n",
    "    # train\n",
    "    set_random_seed(0, deterministic=True)\n",
    "    os.makedirs(cfg.work_dir, exist_ok=True)\n",
    "    train_detector(\n",
    "      cfg.model, \n",
    "      [build_dataset(cfg.data.train)],\n",
    "      cfg, \n",
    "      distributed=False, \n",
    "      validate=True\n",
    "    )\n",
    "\n",
    "# loop\n",
    "for exp in EXPERIMENTS:\n",
    "    print(f\"--- Running {exp['name']} ---\")\n",
    "    train_experiment(exp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d145f",
   "metadata": {},
   "source": [
    "## 7. Inference & Single-Model JSON Exports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99816d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_and_save(exp):\n",
    "    ckpt = os.path.join(CFG[\"WORK_DIR\"], exp[\"name\"], \"latest.pth\")\n",
    "    model = init_detector(exp[\"mmdet_cfg\"], ckpt, device=\"cuda:0\")\n",
    "    results = inference_detector(model, CFG[\"IMAGE_FOLDER\"])\n",
    "    # convert to COCO-style list of dicts\n",
    "    dets = []\n",
    "    for img_id, preds in enumerate(results, start=1):\n",
    "        for cls_id, bboxes in enumerate(preds, start=1):\n",
    "            for x1,y1,x2,y2,score in bboxes:\n",
    "                dets.append({\n",
    "                  \"image_id\":img_id,\n",
    "                  \"category_id\":cls_id,\n",
    "                  \"bbox\":[x1,y1,x2-x1,y2-y1],\n",
    "                  \"score\": score\n",
    "                })\n",
    "    out_json = f\"{CFG['WORK_DIR']}/{exp['name']}/results.json\"\n",
    "    with open(out_json,\"w\") as f: json.dump(dets,f)\n",
    "    return out_json\n",
    "\n",
    "json_files = [infer_and_save(exp) for exp in EXPERIMENTS]\n",
    "print(\"Generated JSONs:\", json_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7703063",
   "metadata": {},
   "source": [
    "## 8. Ensembling & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_small_boxes(detections, factor):\n",
    "    by_img = {}\n",
    "    for d in detections:\n",
    "        by_img.setdefault(d[\"image_id\"], []).append(d)\n",
    "    out=[]\n",
    "    for img_id, ds in by_img.items():\n",
    "        max_area = max([b[\"bbox\"][2]*b[\"bbox\"][3] for b in ds])\n",
    "        out += [d for d in ds if d[\"bbox\"][2]*d[\"bbox\"][3] >= max_area*factor]\n",
    "    return out\n",
    "\n",
    "# gather per-image lists for WBF\n",
    "all_model_boxes, all_model_scores, all_model_labels = {},{},{}\n",
    "for fpath in json_files:\n",
    "    dets = json.load(open(fpath))\n",
    "    filt = filter_small_boxes(dets, CFG[\"BOX_SMALL_FACTOR\"])\n",
    "    tmp = {}\n",
    "    for d in filt:\n",
    "        img = d[\"image_id\"]\n",
    "        all_model_boxes.setdefault(img, []).append([[*d[\"bbox\"]]])\n",
    "        all_model_scores.setdefault(img, []).append([d[\"score\"]])\n",
    "        all_model_labels.setdefault(img, []).append([d[\"category_id\"]-1])  # zero-based\n",
    "\n",
    "# fuse per image\n",
    "ensemble = []\n",
    "for img_id in all_model_boxes:\n",
    "    boxes, scores, labels = weighted_boxes_fusion(\n",
    "        all_model_boxes[img_id],\n",
    "        all_model_scores[img_id],\n",
    "        all_model_labels[img_id],\n",
    "        iou_thr=CFG[\"ENSEMBLE_IOU_THR\"],\n",
    "        skip_box_thr=CFG[\"SCORE_THR\"]\n",
    "    )\n",
    "    for b, s, l in zip(boxes, scores, labels):\n",
    "        ensemble.append({\n",
    "          \"image_id\": img_id,\n",
    "          \"category_id\": int(l+1),\n",
    "          \"bbox\": [*b],\n",
    "          \"score\": float(s)\n",
    "        })\n",
    "\n",
    "# pivot to submission\n",
    "sub_df = pd.read_csv(CFG[\"SAMPLE_SUB\"])\n",
    "counts = pd.DataFrame(ensemble).groupby([\"image_id\",\"category_id\"]).size().reset_index(name=\"count\")\n",
    "# Map image_id back to file name\n",
    "id_to_name = {i+1:str(img)+\".tif\" for i,img in enumerate(pd.read_csv(CFG[\"CSV_TEST\"])[\"ImageId\"])}\n",
    "counts[\"ImageId\"] = counts[\"image_id\"].map(id_to_name)\n",
    "# fill sample submission\n",
    "for _, row in counts.iterrows():\n",
    "    col = f\"{row['category_id']}\"\n",
    "    sub_df.loc[sub_df.ImageId==row.ImageId, col] = row[\"count\"]\n",
    "sub_df.to_csv(\"final_submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
