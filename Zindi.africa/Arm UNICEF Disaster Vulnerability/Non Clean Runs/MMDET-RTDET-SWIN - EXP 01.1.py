HOME = '/notebooks/ZINDI'
print("HOME:", HOME)

%cd {HOME}

%pip install -U -q openmim

!mim install -q "mmengine"
!mim install -q "mmcv"
!mim install -q "mmdet"

#########    !git clone https://github.com/open-mmlab/mmyolo.git




%cd {HOME}/mmdetection
!pip install -v -e .
# "-v" means verbose, or more output
# "-e" means installing a project in editable mode,
# thus any local modifications made to the code will take effect without reinstallation.

# #########    !git clone https://github.com/open-mmlab/mmyolo.git

%cd {HOME}/mmyolo
%pip install -e .

## INFERENCE 
!pip install -q supervision
!pip install -U  sahi

## OPTIONAL
#!mim install "MMDeploy==1.0.0"

## TEST mmYOLO Installation
!mim install "mmyolo"
!mim download mmyolo --config yolov5_s-v61_syncbn_fast_8xb16-300e_coco --dest .


!nvidia-smi


import ast
from PIL import Image, ImageDraw
import pandas as pd
import matplotlib.pyplot as plt
#import pytorch_optimizer as optim2  ## OTHER OPTIMIZER Library

%matplotlib inline
import matplotlib.patches as patches


import warnings
warnings.filterwarnings('ignore')

import cv2
import os
import json
import torch
import random
import numpy as np

from mmdet.apis import init_detector, inference_detector


from mmyolo.utils import register_all_modules
register_all_modules()



# import wandb
# # 85df3c23c14e79994e57996fc5caaa8162d75627


# project_name = "MMDET-OBJECT"
# experiment_name = "EXP01-Take 3 Continue"
# wandb.init(project= project_name, name= experiment_name,reinit=True)

HOME = '/notebooks/ZINDI'
print("HOME:", HOME)


class Config:
    def __init__(self):
        pass


    COCO_DATASET = "/notebooks/ZINDI/Train_COCO_v2.json"
    DATA_FOLDER = '/notebooks/ZINDI'
    TEST_CSV = '/notebooks/ZINDI/Test.csv'
    IMAGE_FOLDER = '/notebooks/ZINDI/Images'
    TRAIN_CSV = "/notebooks/ZINDI/Train.csv"
    #labels_dict = {1: "Other", 2: "Tin", 3: "Thatch", 0:  "background"}
    labels_dict = {1: "Other", 2: "Tin", 3: "Thatch" }

    BATCH_SIZE = 16
    MAX_EPOCHS = 100
    img_size = 640

    convert_coco = False
    split_coco = False
    DEBUG = False


cfg = Config()




# ## Weights

# !mkdir -p {HOME}/weights
# !wget -P {HOME}/weights -q https://download.openmmlab.com/mmyolo/v0/rtmdet/rtmdet_l_syncbn_fast_8xb32-300e_coco/rtmdet_l_syncbn_fast_8xb32-300e_coco_20230102_135928-ee3abdc4.pth
# !ls -lh {HOME}/weights

## Configurations Init 
CONFIG_PATH = f"{HOME}/mmyolo/configs/ppyoloe/ppyoloe_x_fast_8xb16-300e_coco.py"
WEIGHTS_PATH = f"{HOME}/weights/ppyoloe_plus_x_fast_8xb8-80e_coco_20230104_194921-8c953949.pth"


## Model
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model = init_detector(CONFIG_PATH, WEIGHTS_PATH, device=DEVICE)




if cfg.convert_coco:
    train = pd.read_csv(cfg.TRAIN_CSV, header=0)
    train = train.dropna() 

    train['bbox'] = train['bbox'].apply(ast.literal_eval)

    # Initialize the COCO format dictionary
    coco_format = {
        "images": [],
        "annotations": [],
        "categories": [
            {"id": 1, "name": "Other"},
            {"id": 2, "name": "Tin"},
            {"id": 3, "name": "Thatch"}
        ]
    }
    index_dict = {}
    i = 0
    unique_image_ids = train['image_id'].unique()
    for image_id in unique_image_ids:
        index_dict[image_id] = i
        i += 1


    # Helper function to get image dimensions
    def get_image_dimensions(image_path):
        with Image.open(image_path) as img:
            return img.width, img.height


    # Convert bbox to segmentation format
    def bbox_to_segmentation(bbox):
        x, y, width, height = bbox
        return [x, y, x + width, y, x + width, y + height, x, y + height]


    # Populate images section
    unique_image_ids = train['image_id'].unique()
    for image_id in unique_image_ids:
        file_name = f"{image_id}.tif"  # Adjust the extension if needed
        image_path = os.path.join(cfg.IMAGE_FOLDER, file_name)
        width, height = get_image_dimensions(image_path)
        coco_format["images"].append({
            "id": index_dict[image_id],
            "file_name": file_name,
            "width": width,
            "height": height
        })

    # Annotations
    for index, row in train.iterrows():
        area = row["bbox"][2] * row["bbox"][3]  # width * height
        if area > 0:
            annotation = {
                "id": index,
                "image_id": index_dict[row["image_id"]],
                "category_id": int(row["category_id"]),
                "bbox": row["bbox"],
                "area": area,
                "iscrowd": 0,
                #"segmentation": []  # Empty if not using segmentation
                "segmentation": [bbox_to_segmentation(row["bbox"])]
            }
            coco_format["annotations"].append(annotation)

    # Save to JSON
    with open(cfg.COCO_DATASET, 'w') as f:
        json.dump(coco_format, f, indent=4)


if cfg.split_coco:

    import json
    import os
    import shutil
    import random
    from sklearn.model_selection import train_test_split

    # Function to read the COCO format JSON file
    # Function to read the COCO format JSON file
    def read_coco_json(file_path):
        with open(file_path, 'r') as f:
            data = json.load(f)
        return data

    # Function to split the data while stratifying by class
    def stratified_split(coco_data, test_size=0.1):
        annotations = coco_data['annotations']
        images = coco_data['images']
        categories = coco_data['categories']

        # Create a dictionary mapping image_id to file_name
        image_id_to_filename = {image['id']: image['file_name'].lower() for image in images}

        # Create a list of tuples (image_id, category_id)
        image_id_category_pairs = [(anno['image_id'], anno['category_id']) for anno in annotations]

        # Split the data
        train_ids, valid_ids = train_test_split(
            image_id_category_pairs,
            test_size=test_size,
            stratify=[pair[1] for pair in image_id_category_pairs]
        )

        return train_ids, valid_ids, image_id_to_filename

    # Function to create directories and move files
    def move_files(train_ids, valid_ids, image_id_to_filename, source_dir, dest_dir):
        os.makedirs(os.path.join(dest_dir, 'train'), exist_ok=True)
        os.makedirs(os.path.join(dest_dir, 'valid'), exist_ok=True)

        train_ids_final = []
        valid_ids_final = []

        for image_id, _ in train_ids:
            file_name = image_id_to_filename[image_id]
            source_file = os.path.join(source_dir, file_name)
            if os.path.exists(source_file):
                shutil.move(source_file, os.path.join(dest_dir, 'train', file_name))
                train_ids_final.append((image_id, _))

        for image_id, _ in valid_ids:
            file_name = image_id_to_filename[image_id]
            source_file = os.path.join(source_dir, file_name)
            if os.path.exists(source_file):
                shutil.move(source_file, os.path.join(dest_dir, 'valid', file_name))
                valid_ids_final.append((image_id, _))

        return train_ids_final, valid_ids_final

    # Function to write the COCO JSON file
    def write_coco_json(coco_data, ids, output_file):
        images = [img for img in coco_data['images'] if img['id'] in [id for id, _ in ids]]
        annotations = [anno for anno in coco_data['annotations'] if anno['image_id'] in [id for id, _ in ids]]
        data = {
            'images': images,
            'annotations': annotations,
            'categories': coco_data['categories']
        }
        with open(output_file, 'w') as f:
            json.dump(data, f)





    coco_file_path = cfg.COCO_DATASET
    source_dir = cfg.IMAGE_FOLDER
    dest_dir = cfg.DATA_FOLDER

    # Step 1: Read the COCO format JSON file
    coco_data = read_coco_json(coco_file_path)

    # Step 2: Split the data
    train_ids, valid_ids, image_id_to_filename = stratified_split(coco_data, test_size=0.1)

    # Step 3: Create directories and move files
    train_ids_final, valid_ids_final = move_files(train_ids, valid_ids, image_id_to_filename, source_dir, dest_dir)

    # Step 4: Write the COCO JSON files
    write_coco_json(coco_data, train_ids_final, os.path.join(dest_dir, 'train/_annotations.coco.json'))
    write_coco_json(coco_data, valid_ids_final, os.path.join(dest_dir, 'valid/_annotations.coco.json'))



import json

def validate_coco_json(coco_json):
    # Load JSON
    with open(coco_json, 'r') as f:
        data = json.load(f)

    # Check images
    image_ids = {image['id'] for image in data['images']}
    if not image_ids:
        print("No images found.")
        return False

    # Check categories
    category_ids = {category['id'] for category in data['categories']}
    if not category_ids:
        print("No categories found.")
        return False

    # Check annotations
    for annotation in data['annotations']:
        if annotation['image_id'] not in image_ids:
            print(f"Annotation {annotation['id']} references non-existent image_id {annotation['image_id']}.")
            return False
        if annotation['category_id'] not in category_ids:
            print(f"Annotation {annotation['id']} references non-existent category_id {annotation['category_id']}.")
            return False

        # Check bbox and segmentation
        bbox = annotation['bbox']
        if len(bbox) != 4:
            print(f"Annotation {annotation['id']} has invalid bbox.")
            return False
        segmentation = annotation['segmentation']
        if not isinstance(segmentation, list) or not all(isinstance(seg, list) for seg in segmentation):
            print(f"Annotation {annotation['id']} has invalid segmentation.")
            return False

    print("JSON is valid.")
    return True

# Validate the provided JSON file
validate_coco_json('/notebooks/ZINDI/train/_annotations.coco.json')


print(tuple(sorted(cfg.labels_dict.values())))
print(len(cfg.labels_dict))

#hook_wandb = {'project': project_name}

cfg.MAX_EPOCHS = 300


cfg.DATA_FOLDER

class_name = {tuple(sorted(cfg.labels_dict.values()))}
class_name

CUSTOM_CONFIG_PATH = f"{HOME}/mmyolo/configs/yolov8/custom.py"

CUSTOM_CONFIG = f"""
_base_ = ['../_base_/default_runtime.py', '../_base_/det_p5_tta.py', './yolov8_n_syncbn_fast_8xb16-500e_coco.py']

# ========================Frequently modified parameters======================
# -----data related-----
data_root = '{cfg.DATA_FOLDER}/'



resume = True


train_ann_file = 'train/_annotations.coco.json'
train_data_prefix = 'train/'

val_ann_file = 'valid/_annotations.coco.json'
val_data_prefix = 'valid/'


test_ann_file = 'Test_COCO_v2.json'
test_data_prefix = 'Images/'

class_name = {tuple(sorted(cfg.labels_dict.values()))}
num_classes = {len(cfg.labels_dict)}

metainfo = dict(classes=class_name, palette=[(20, 220, 60)])

train_batch_size_per_gpu = {cfg.BATCH_SIZE}
# Worker to pre-fetch data for each single GPU during training
train_num_workers = 8
# persistent_workers must be False if num_workers is 0.
persistent_workers = True

# -----train val related-----
# Base learning rate for optim_wrapper. Corresponding to 8xb16=64 bs
base_lr = 0.004
max_epochs = {cfg.MAX_EPOCHS}  # Maximum training epochs
# Change train_pipeline for final 20 epochs (stage 2)
num_epochs_stage2 = 30




model_test_cfg = dict(
    # The config of multi-label for multi-class prediction.
    multi_label=True,
    # The number of boxes before NMS
    nms_pre=30000,
    score_thr=0.001,  # Threshold to filter out boxes.
    nms=dict(type='nms', iou_threshold=0.65),  # NMS type and threshold
    max_per_img=300)  # Max number of detections of each image
# ========================modified parameters======================
use_mask2refine = True
min_area_ratio = 0.01  # YOLOv5RandomAffine

# ===============================Unmodified in most cases====================
pre_transform = [
    dict(type='LoadImageFromFile', backend_args=_base_.backend_args),
    dict(
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True,
        mask2bbox=use_mask2refine)
]

last_transform = [
    # Delete gt_masks to avoid more computation
    dict(type='RemoveDataElement', keys=['gt_masks']),
    dict(
        type='mmdet.Albu',
        transforms=_base_.albu_train_transforms,
        bbox_params=dict(
            type='BboxParams',
            format='pascal_voc',
            label_fields=['gt_bboxes_labels', 'gt_ignore_flags']),
        keymap={
            'img': 'image',
            'gt_bboxes': 'bboxes'
        }),
    dict(type='YOLOv5HSVRandomAug'),
    dict(type='mmdet.RandomFlip', prob=0.5),
    dict(
        type='mmdet.PackDetInputs',
        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'flip',
                   'flip_direction'))
]




mixup_prob = 0.1
copypaste_prob = 0.1

# ===============================Unmodified in most cases====================
img_scale = _base_.img_scale
pre_transform = _base_.pre_transform
last_transform = _base_.last_transform

model = dict(
    backbone=dict(
        last_stage_out_channels=last_stage_out_channels,
        deepen_factor=deepen_factor,
        widen_factor=widen_factor),
    neck=dict(
        deepen_factor=deepen_factor,
        widen_factor=widen_factor,
        in_channels=[256, 512, last_stage_out_channels],
        out_channels=[256, 512, last_stage_out_channels]),
    bbox_head=dict(
        head_module=dict(
            widen_factor=widen_factor,
            in_channels=[256, 512, last_stage_out_channels])))

mosaic_affine_transform = [
    dict(
        type='Mosaic',
        img_scale=img_scale,
        pad_val=114.0,
        pre_transform=pre_transform),
    dict(type='YOLOv5CopyPaste', prob=copypaste_prob),
    dict(
        type='YOLOv5RandomAffine',
        max_rotate_degree=0.0,
        max_shear_degree=0.0,
        max_aspect_ratio=100.,
        scaling_ratio_range=(1 - affine_scale, 1 + affine_scale),
        # img_scale is (width, height)
        border=(-img_scale[0] // 2, -img_scale[1] // 2),
        border_val=(114, 114, 114),
        min_area_ratio=_base_.min_area_ratio,
        use_mask_refine=_base_.use_mask2refine)
]

train_pipeline = [
    *pre_transform, *mosaic_affine_transform,
    dict(
        type='YOLOv5MixUp',
        prob=mixup_prob,
        pre_transform=[*pre_transform, *mosaic_affine_transform]),
    *last_transform
]

train_pipeline_stage2 = [
    *pre_transform,
    dict(type='YOLOv5KeepRatioResize', scale=img_scale),
    dict(
        type='LetterResize',
        scale=img_scale,
        allow_scale_up=True,
        pad_val=dict(img=114.0)),
    dict(
        type='YOLOv5RandomAffine',
        max_rotate_degree=0.0,
        max_shear_degree=0.0,
        scaling_ratio_range=(1 - affine_scale, 1 + affine_scale),
        max_aspect_ratio=_base_.max_aspect_ratio,
        border_val=(114, 114, 114),
        min_area_ratio=_base_.min_area_ratio,
        use_mask_refine=_base_.use_mask2refine), *last_transform
]

train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
_base_.custom_hooks[1].switch_pipeline = train_pipeline_stage2



last_stage_out_channels = 512

mixup_prob = 0.15
copypaste_prob = 0.3

# =======================Unmodified in most cases==================
img_scale = _base_.img_scale
pre_transform = _base_.pre_transform
last_transform = _base_.last_transform
affine_scale = _base_.affine_scale

model = dict(
    backbone=dict(
        last_stage_out_channels=last_stage_out_channels,
        deepen_factor=deepen_factor,
        widen_factor=widen_factor),
    neck=dict(
        deepen_factor=deepen_factor,
        widen_factor=widen_factor,
        in_channels=[256, 512, last_stage_out_channels],
        out_channels=[256, 512, last_stage_out_channels]),
    bbox_head=dict(
        head_module=dict(
            widen_factor=widen_factor,
            in_channels=[256, 512, last_stage_out_channels])))

mosaic_affine_transform = [
    dict(
        type='Mosaic',
        img_scale=img_scale,
        pad_val=114.0,
        pre_transform=pre_transform),
    dict(type='YOLOv5CopyPaste', prob=copypaste_prob),
    dict(
        type='YOLOv5RandomAffine',
        max_rotate_degree=0.0,
        max_shear_degree=0.0,
        max_aspect_ratio=100.,
        scaling_ratio_range=(1 - affine_scale, 1 + affine_scale),
        # img_scale is (width, height)
        border=(-img_scale[0] // 2, -img_scale[1] // 2),
        border_val=(114, 114, 114),
        min_area_ratio=_base_.min_area_ratio,
        use_mask_refine=_base_.use_mask2refine)
]

train_pipeline = [
    *pre_transform, *mosaic_affine_transform,
    dict(
        type='YOLOv5MixUp',
        prob=mixup_prob,
        pre_transform=[*pre_transform, *mosaic_affine_transform]),
    *last_transform
]

train_dataloader = dict(dataset=dict(pipeline=train_pipeline))



deepen_factor = 1.00
widen_factor = 1.25

model = dict(
    backbone=dict(deepen_factor=deepen_factor, widen_factor=widen_factor),
    neck=dict(deepen_factor=deepen_factor, widen_factor=widen_factor),
    bbox_head=dict(head_module=dict(widen_factor=widen_factor)))















# ========================Possible modified parameters========================
# -----data related-----
img_scale = ({cfg.img_size}, {cfg.img_size})  # width, height
# ratio range for random resize
random_resize_ratio_range = (0.1, 2.0)
# Cached images number in mosaic
mosaic_max_cached_images = 40
# Number of cached images in mixup
mixup_max_cached_images = 20
# Dataset type, this will be used to define the dataset
dataset_type = 'YOLOv5CocoDataset'
# Batch size of a single GPU during validation
val_batch_size_per_gpu = 32
# Worker to pre-fetch data for each single GPU during validation
val_num_workers = 8

# Config of batch shapes. Only on val.
batch_shapes_cfg = dict(
    type='BatchShapePolicy',
    batch_size=val_batch_size_per_gpu,
    img_size=img_scale[0],
    size_divisor=32,
    extra_pad_ratio=0.5)

# -----model related-----
# The scaling factor that controls the depth of the network structure
deepen_factor = 1.0
# The scaling factor that controls the width of the network structure
widen_factor = 1.0
# Strides of multi-scale prior box
strides = [8, 16, 32]

norm_cfg = dict(type='BN')  # Normalization config

# -----train val related-----
lr_start_factor = 1.0e-5
dsl_topk = 13  # Number of bbox selected in each level
loss_cls_weight = 1.0
loss_bbox_weight = 2.0
qfl_beta = 2.0  # beta of QualityFocalLoss
weight_decay = 0.05

# Save model checkpoint and validation intervals
save_checkpoint_intervals = 1
# validation intervals in stage 2
val_interval_stage2 = 1
# The maximum checkpoints to keep.
max_keep_ckpts = 3
# single-scale training is recommended to
# be turned on, which can speed up training.
env_cfg = dict(cudnn_benchmark=True)

# ===============================Unmodified in most cases====================
# https://mmengine.readthedocs.io/en/latest/api/visualization.html

train_dataloader = dict(
    batch_size=train_batch_size_per_gpu,
    num_workers=train_num_workers,
    persistent_workers=persistent_workers,
    pin_memory=True,
    collate_fn=dict(type='yolov5_collate'),
    sampler=dict(type='DefaultSampler', shuffle=True),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        metainfo=metainfo,
        ann_file=train_ann_file,
        data_prefix=dict(img=train_data_prefix),
        filter_cfg=dict(filter_empty_gt=True, min_size=32),
        pipeline=train_pipeline))

val_dataloader = dict(
    batch_size=val_batch_size_per_gpu,
    num_workers=val_num_workers,
    persistent_workers=persistent_workers,
    pin_memory=True,
    drop_last=False,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        metainfo=metainfo,
        ann_file=val_ann_file,
        data_prefix=dict(img=val_data_prefix),
        test_mode=True,
        batch_shapes_cfg=batch_shapes_cfg,
        pipeline=test_pipeline))

test_dataloader = dict(
    batch_size=val_batch_size_per_gpu,
    num_workers=val_num_workers,
    persistent_workers=persistent_workers,
    pin_memory=True,
    drop_last=False,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        metainfo=metainfo,
        ann_file=test_ann_file,
        data_prefix=dict(img=test_data_prefix),
        test_mode=True,
        batch_shapes_cfg=batch_shapes_cfg,
        pipeline=test_pipeline))

# Reduce evaluation time
val_evaluator = dict(
    type='mmdet.CocoMetric',
    proposal_nums=(100, 1, 10),
    ann_file=data_root + val_ann_file,
    metric='bbox')

test_evaluator = dict(
    type='mmdet.CocoMetric',
    ann_file=data_root + 'Test_COCO_v2.json',
    metric=['bbox'],
    format_only=True, 
    outfile_prefix= data_root +'/InternImage/detection/work_dirs')  

# optimizer
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=weight_decay),
    paramwise_cfg=dict(
        norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))

# learning rate
param_scheduler = [
    dict(
        type='LinearLR',
        start_factor=lr_start_factor,
        by_epoch=False,
        begin=0,
        end=1000),
    dict(
        # use cosine lr from 150 to 300 epoch
        type='CosineAnnealingLR',
        eta_min=base_lr * 0.05,
        begin=max_epochs // 2,
        end=max_epochs,
        T_max=max_epochs // 2,
        by_epoch=True,
        convert_to_iter_based=True),
]

train_cfg = dict(
    type='EpochBasedTrainLoop',
    max_epochs=max_epochs,
    val_interval=save_checkpoint_intervals,
    dynamic_intervals=[(max_epochs - num_epochs_stage2, val_interval_stage2)])

val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')
"""
     
with open(CUSTOM_CONFIG_PATH, 'w') as file:
    file.write(CUSTOM_CONFIG)


HOME = '/notebooks/ZINDI'
print("HOME:", HOME)

%cd {HOME}/mmdetection
!python tools/train.py /notebooks/ZINDI/mmdetection/configs/rtmdet/rtmdet_l_swin_b_p6_4xb16-100e_coco.py


%cd {HOME}/mmdetection
!python tools/train.py /notebooks/ZINDI/mmdetection/configs/ddq/ddq-detr-4scale_swinl_8xb2-30e_coco.py





HOME = '/notebooks/ZINDI'
print("HOME:", HOME)

%cd {HOME}/mmdetection
!python tools/test.py \
    /notebooks/ZINDI/mmdetection/configs/ddq/ddq-detr-4scale_swinl_8xb2-30e_coco-LOW_IOU.py \
   /notebooks/ZINDI/mmdetection/work_dirs/ddq-detr-4scale_swinl_8xb2-30e_coco/epoch_19.pth \
    --work-dir /notebooks/ZINDI/mmdetection/work_dirs/ddq-detr-4scale_swinl_8xb2-30e_coco \
    --cfg-options "jsonfile_prefix=/notebooks/ZINDI/mmdetection/work_dirs/ddq-detr-4scale_swinl_8xb2-30e_coco/RESULTS_11.json" \
    --tta


# %cd {HOME}/mmyolo
# !python tools/test.py \
#     /notebooks/ZINDI/mmyolo/configs/yolov8/yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco.py \
#     /notebooks/ZINDI/mmyolo/work_dirs/yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco/best_coco_bbox_mAP_epoch_204.pth \
#     --work-dir /notebooks/ZINDI/mmyolo/work_dirs/yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco \
#     --cfg-options "jsonfile_prefix=/notebooks/ZINDI/mmyolo/work_dirs/yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco" \
#     ##--tta


import json

def filter_small_boxes_from_json(input_json_file,  box_too_small_factor = 0.025):
    """
    Filter out bounding boxes that are too small compared to the largest bounding box in each image from a JSON file.

    Args:
        input_json_file (str): Path to the input JSON file containing detection results.
        output_json_file (str): Path to the output JSON file to save filtered results.
        box_too_small_factor (float): Factor to determine the size threshold for small boxes.
    """
    with open(input_json_file, 'r') as file:
        detections = json.load(file)

    filtered_detections = []

    # Group detections by image_id
    grouped_detections = {}
    for detection in detections:
        image_id = detection['image_id']
        if image_id not in grouped_detections:
            grouped_detections[image_id] = []
        grouped_detections[image_id].append(detection)

    # Process each group of detections
    for image_id, image_detections in grouped_detections.items():
        if len(image_detections) == 0:
            continue

        box_areas = [det['bbox'][2] * det['bbox'][3] for det in image_detections]  # Assuming bbox format is [x, y, width, height]
        largest_area = max(box_areas)
        filtered_image_detections = []

        for idx, detection in enumerate(image_detections):
            if box_areas[idx] >= largest_area * box_too_small_factor:
                filtered_image_detections.append(detection)

        filtered_detections.extend(filtered_image_detections)

    return filtered_detections



import json
import pandas as pd


# Load JSON files

target_json = '/notebooks/ZINDI/mmyolo/results-PseudoV1-TTA-S.json'

# with open(target_json) as f:
#     annotations = json.load(f)


annotations = filter_small_boxes_from_json(target_json)


with open('/notebooks/ZINDI/Test_COCO_v2.json') as f:
    image_info = json.load(f)

# Load sample submission file
sample_submission = pd.read_csv('/notebooks/ZINDI/SampleSubmission.csv')




# Minimum score threshold
threshold = 0.45

# Create a mapping of image_id to file_name
image_id_to_name = {image['id']: image['file_name'] for image in image_info['images']}

# Filter annotations based on the threshold
filtered_annotations = [anno for anno in annotations if anno['score'] >= threshold]

# Prepare the dataframe
data = []
for anno in filtered_annotations:
    try:
        file_name = image_id_to_name[anno['image_id']]
        base_name = file_name.rsplit('.', 1)[0]  # Remove file extension
        category_id = anno['category_id']
        data.append({'image_id': base_name, 'category_id': category_id})
    except:
        print("error in :", anno['image_id'])

df = pd.DataFrame(data)
df['count'] = 1

# Pivot table to get counts of each category for each image
pivot_df = df.pivot_table(index='image_id', columns='category_id', values='count', aggfunc='sum', fill_value=0).reset_index()

# Add missing category columns if not present in pivot table
for category in image_info['categories']:
    if category['id'] not in pivot_df.columns:
        pivot_df[category['id']] = 0

# Create final output with counts
output = []
for _, row in pivot_df.iterrows():
    image_id = row['image_id']
    for category in image_info['categories']:
        category_id = category['id']
        target = row[category_id]
        output.append({'image_id': f"{image_id}_{category_id}", 'Target': target})

# Convert to final dataframe
final_df = pd.DataFrame(output)
print(final_df.sum())
print(final_df.shape)
final_df.head(10)


# Left join with sample submission file
merged_df = sample_submission[['image_id']].merge(final_df, on='image_id', how='left')

# Fill NaN values with 0 in 'Target' column
merged_df['Target'] = merged_df['Target'].fillna(0).astype(int)

print(merged_df.Target.sum())
# Display the merged dataframe
merged_df.head(10)

# Save the final output to a CSV file
merged_df.to_csv(f'/notebooks/Pseudo-V1-threshold-{threshold}.csv', index=False)



%cd /notebooks/ZINDI/mmdetection
!python tools/analysis_tools/fuse_results.py \
    /notebooks/ZINDI/mmdet_outputs/results-RTDET-V1.3-EP70-bbox.json \
    /notebooks/ZINDI/mmdet_outputs/results-InternImage-Best-EP-8.bbox.json \
    # /notebooks/ZINDI/mmdet_outputs/results-Internimage-Best-EP-34.bbox.json \
    # /notebooks/ZINDI/mmdet_outputs/results-RTDET-V1.0-EP-100.json \
    --weights 1 1  \
    --save_fusion_results \
    --out_dir /notebooks/ZINDI/mmdet_outputs
        


test = pd.read_csv('/notebooks/ZINDI/Test.csv')
test.head(2)

submission  = pd.read_csv('/notebooks/ZINDI/SampleSubmission.csv')
submission.head(2)

pip install supervision==0.21


from mmdet.apis import init_detector, inference_detector
import cv2
import supervision as sv




config_path = f"{HOME}/mmyolo/work_dirs/custom/custom.py"
model_path = f"{HOME}/mmyolo/work_dirs/custom/epoch_335.pth"

model = init_detector(config_path, model_path, device='cuda:0')


#labels_dict = {1: "Other", 2: "Tin", 3: "Thatch" }
labels_dict__ = {3: "Other", 1: "Tin", 2: "Thatch" }


target_ = 'id_gbhtfm03gku3'
image_file = f"/notebooks/ZINDI/train/{target_}.tif" # image or folder path
image = cv2.imread(image_file)

result = inference_detector(model, image)
detections = sv.Detections.from_mmdetection(result)
detections = detections[detections.confidence > 0.4].with_nms()

box_annotator = sv.BoxAnnotator()


labels = [
    f"{labels_dict__[class_id]} {confidence:0.2f}"
    for _, _, confidence, class_id, _ , _
    in detections
]

counter = {'Other': 0 , 'Tin': 0 , 'Thatch': 0 ,}
for _, _, confidence, class_id, _, _ in detections:
    counter[labels_dict__[class_id]] += 1

annotated_image = box_annotator.annotate(image.copy(), detections, labels=labels)
print(counter)
sv.plot_image(image=annotated_image, size=(10, 10))

# ds = sv.DetectionDataset.from_coco(
#     images_directory_path=f"{dataset.location}/test",
#     annotations_path=f"{dataset.location}/test/_annotations.coco.json",
# )

# images = list(ds.images.values())

import supervision as sv
import numpy as np
from supervision.detection.tools import inference_slicer
import cv2
from mmdet.apis import init_detector, inference_detector


from mmyolo.utils import register_all_modules
register_all_modules()

HOME = '/notebooks/ZINDI'
print("HOME:", HOME)



# config_path = f"{HOME}/mmyolo/work_dirs/custom/custom_.py"
# model_path = f"{HOME}/mmyolo/work_dirs/custom/epoch_335.pth"

config_path = '/notebooks/ZINDI/mmyolo/configs/yolov8/yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco.py'
model_path=    '/notebooks/ZINDI/mmyolo/work_dirs/yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco/epoch_300.pth'

model = init_detector(config_path, model_path, device='cuda:0')


'''
Thres: 0.50:
    - Count > 40
        Thres: 0.45
            - Count >= 50
                Final Thres: == 0.40
            Else:
                Thres = Same @ 0.45
    

Eliminate by BBOX Area
< Threshold of False Positive,,, Collect False Positive, Get Mean of Area, 
Eliminate by area Less

'''


target_ = 'id_0k31nj2gxrn7'
image_file = f"/notebooks/ZINDI/train/{target_}.tif" # image or folder path
image = cv2.imread(image_file)


## RAW

result = inference_detector(model, image)
#result.get('pred_instances')

results = result.get('pred_instances')
ms = results['scores'] >= 0.40
results['scores'][ms]
print(len(results['scores'][ms]))

from mmengine.structures import InstanceData
CONFIDENCE_THRESHOLD = 0.45
NMS_IOU_THRESHOLD = 0.70


area_flag = False
area_thres = 500

def callback(image_slice: np.ndarray) -> sv.Detections:
    #results = inference_detector(model, image_slice).get('pred_instances')#[0]
    #results = model.infer(image_slice)[0]
    #return sv.Detections.from_inference(results)

    result = inference_detector(model, image)

    if area_flag:
        # Calculate areas
        bboxes = result.get('pred_instances')['bboxes']
        areas = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])
        result.get('pred_instances')['areas'] = areas

        results = result.get('pred_instances')

        # Create a mask to filter elements based on the area threshold
        mask = results['areas'] >= area_thres

        # Create an InstanceData object and assign the filtered data to it
        filtered_instance_data = InstanceData(
            bboxes=results['bboxes'][mask],
            scores=results['scores'][mask],
            labels=results['labels'][mask],
            areas=results['areas'][mask]
        )
        result.pred_instances = filtered_instance_data
    detections = sv.Detections.from_mmdetection(result)
    #print(type(result))
    clean = detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)
    
    return clean


slicer = inference_slicer.InferenceSlicer(callback=callback, slice_wh= (1024,1024), overlap_ratio_wh = (0.01, 0.01), iou_threshold = NMS_IOU_THRESHOLD)
sliced_detections = slicer(image=image)


labels_dict__ = {3: "Other", 1: "Tin", 2: "Thatch", 0 : "Background" }

counter = {'Other': 0 , 'Tin': 0 , 'Thatch': 0 , "Background" : 0}
for clss in sliced_detections.class_id:
    counter[labels_dict__[clss]] += 1
print(counter)

# CONFIDENCE_THRESHOLD = 0.50
# NMS_IOU_THRESHOLD = 0.20

# def callback(image_slice: np.ndarray) -> sv.Detections:

#     result = inference_detector(model, image)
#     detections = sv.Detections.from_mmdetection(result)
#     print(type(result))
#     clean = detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)
#     print(clean)
#     return clean


# slicer = inference_slicer.InferenceSlicer(callback=callback, slice_wh= (4000,4000), overlap_ratio_wh = (0.50, 0.50), iou_threshold = NMS_IOU_THRESHOLD)
# sliced_detections = slicer(image=image)


# labels_dict__ = {3: "Other", 1: "Tin", 2: "Thatch", 0 : "Background" }

# counter = {'Other': 0 , 'Tin': 0 , 'Thatch': 0 , "Background" : 0}
# for clss in sliced_detections.class_id:
#     counter[labels_dict__[clss]] += 1
# print(counter)

label_annotator = sv.LabelAnnotator()
box_annotator = sv.BoxAnnotator()

# You can also use sv.MaskAnnotator() for instance segmentation models
# mask_annotator = sv.MaskAnnotator()

annotated_image = box_annotator.annotate(
    scene=image.copy(), detections=sliced_detections)
# annotated_image = mask_annotator.annotate(
#    scene=image.copy(), detections=sliced_detections)

annotated_image = label_annotator.annotate(
    scene=annotated_image, detections=sliced_detections)

sv.plot_image(annotated_image)

ds = sv.DetectionDataset.from_coco(
    images_directory_path=f"{HOME}/train",
    annotations_path=f"{HOME}/train/_annotations.coco.json",
)

print('dataset classes:', ds.classes)
print('dataset size:', len(ds))


def callback(image: np.ndarray) -> sv.Detections:
    result = inference_detector(model, image)
    detections = sv.Detections.from_mmdetection(result)
    return detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)


CONFIDENCE_THRESHOLD = 0.50
NMS_IOU_THRESHOLD = 0.60

confusion_matrix = sv.ConfusionMatrix.benchmark(
    dataset = ds,
    callback = callback
)

#_ = confusion_matrix.plot()
     

_ = confusion_matrix.plot()

CONFIDENCE_THRESHOLD = 0.45
NMS_IOU_THRESHOLD = 0.60

confusion_matrix = sv.ConfusionMatrix.benchmark(
    dataset = ds,
    callback = callback
)

_ = confusion_matrix.plot()
     

CONFIDENCE_THRESHOLD = 0.40
NMS_IOU_THRESHOLD = 0.60

confusion_matrix = sv.ConfusionMatrix.benchmark(
    dataset = ds,
    callback = callback
)

_ = confusion_matrix.plot()
     

CONFIDENCE_THRESHOLD = 0.60
NMS_IOU_THRESHOLD = 0.60

confusion_matrix = sv.ConfusionMatrix.benchmark(
    dataset = ds,
    callback = callback
)

_ = confusion_matrix.plot()
     

#!pip install -U sahi mmdet mmengine 'mmcv>=2.0.0'

#!pip install mmyolo==0.1.2

#!pip install sahi==0.11.16

from sahi import AutoDetectionModel
from sahi.utils.cv import read_image
from sahi.predict import get_prediction, get_sliced_prediction, predict
from IPython.display import Image

from mmyolo.utils import register_all_modules
register_all_modules()


HOME = '/notebooks/ZINDI'
print("HOME:", HOME)


from mmengine.registry import DefaultScope

default_scope = 'mmyolo'

#config_path = f"{HOME}/mmyolo/configs/rtmdet/custom.py"

config_path = f"/notebooks/ZINDI/mmyolo/configs/yolov8/yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco.py"
model_path = f"/notebooks/ZINDI/mmyolo/work_dirs/yolov8_l_mask-refine_syncbn_fast_8xb16-500e_coco/epoch_300.pth"

#model = init_detector(config_path, model_path, device='cuda:0')

# with DefaultScope.overwrite_default_scope('mmdet'):
#     predictor = init_detector(config_path, model_path)

# get batch predict result
target_ = 'id_bw60pzuanqn5'
result = predict(
    model_type='yolov5', # one of 'yolov5', 'mmdet', 'detectron2'
    model_path= model_path, # path to model weight file
    model_config_path=config_path, # for detectron2 and mmdet models
    model_confidence_threshold=0.5,
    model_device='cuda:0', # or 'cuda:0'
    source=f"/notebooks/ZINDI/Images/{target_}.tif", # image or folder path
    no_standard_prediction=True,
    no_sliced_prediction=False,
    slice_height=320,
    slice_width=320,
    overlap_height_ratio=0.5,
    overlap_width_ratio=0.5,
    export_pickle=False,
    export_crop=False,
)

detection_model = AutoDetectionModel.from_pretrained(
    model_type='mmdet',
    model_path=model_path,
    config_path=config_path,
    confidence_threshold=0.4,
    image_size=640,
    device='cuda:0', # or ('cuda:0', 'cpu')
)

# Display Single Predictions Without SAHI
from IPython.display import Image

target_ = 'id_bw60pzuanqn5'
result = get_prediction(f"/notebooks/ZINDI/Images/{target_}.tif", detection_model)
result.export_visuals(export_dir="/notebooks/ZINDI/inference_demo/")
Image(f"/notebooks/ZINDI/inference_demo/{target_}.png")




