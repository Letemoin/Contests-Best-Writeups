#!/usr/bin/env python
# coding: utf-8

# ## LIB & PIP

# - REF Notebook https://github.com/roboflow/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb

# In[ ]:


HOME = '/notebooks/ZINDI'
print("HOME:", HOME)


# In[ ]:


get_ipython().run_line_magic('cd', '{HOME}')

get_ipython().run_line_magic('pip', 'install -U -q openmim')

get_ipython().system('mim install -q "mmengine"')
get_ipython().system('mim install -q "mmcv"')
get_ipython().system('mim install -q "mmdet"')

#########    !git clone https://github.com/open-mmlab/mmyolo.git

get_ipython().run_line_magic('cd', '{HOME}/mmyolo')

get_ipython().run_line_magic('pip', 'install -e .')


# In[ ]:


# %cd {HOME}

# %pip install -U -q openmim

# !mim install -q "mmengine>=0.6.0"
# !mim install -q "mmcv>=2.0.0rc4,<2.1.0"
# !mim install -q "mmdet>=3.0.0rc6,<3.1.0"

# #########    !git clone https://github.com/open-mmlab/mmyolo.git


# In[ ]:


get_ipython().run_line_magic('cd', '{HOME}/mmyolo')

get_ipython().run_line_magic('pip', 'install -e .')


# In[ ]:


get_ipython().system('mim install -q "mmcv"')


# In[ ]:


## INFERENCE 
get_ipython().system('pip install -q supervision')
get_ipython().system('pip install -U  sahi')


# In[ ]:


## OPTIONAL
#!mim install "MMDeploy==1.0.0"


# In[ ]:


## TEST mmYOLO Installation
get_ipython().system('mim install "mmyolo"')
get_ipython().system('mim download mmyolo --config yolov5_s-v61_syncbn_fast_8xb16-300e_coco --dest .')


# ## IMPORT

# In[ ]:


get_ipython().system('nvidia-smi')


# In[ ]:


import ast
from PIL import Image, ImageDraw
import pandas as pd
import matplotlib.pyplot as plt
#import pytorch_optimizer as optim2  ## OTHER OPTIMIZER Library

get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.patches as patches


import warnings
warnings.filterwarnings('ignore')


# In[ ]:


import cv2
import os
import json
import torch
import random
import numpy as np

from mmdet.apis import init_detector, inference_detector


# In[ ]:


from mmyolo.utils import register_all_modules
register_all_modules()


# In[ ]:





# In[ ]:


# import wandb
# # 85df3c23c14e79994e57996fc5caaa8162d75627


# project_name = "MMDET-OBJECT"
# experiment_name = "EXP01.3"
# wandb.init(project= project_name, name= experiment_name,reinit=True)


# ## DATA

# In[ ]:


HOME = '/notebooks/ZINDI'
print("HOME:", HOME)


# In[ ]:


class Config_manual:
    def __init__(self):
        pass


    COCO_DATASET = "/notebooks/ZINDI/Train_COCO_v2.json"
    DATA_FOLDER = '/notebooks/ZINDI'
    
    IMAGE_FOLDER = '/notebooks/ZINDI/Images'
    TRAIN_CSV = "/notebooks/ZINDI/Train.csv"

    TEST_CSV = '/notebooks/ZINDI/Test.csv'
    TEST_Annotations = "/notebooks/ZINDI/Test_COCO_v2.json"

    #labels_dict = {1: "Other", 2: "Tin", 3: "Thatch", 0:  "background"}
    labels_dict = {1: "Other", 2: "Tin", 3: "Thatch" }

    BATCH_SIZE = 4
    MAX_EPOCHS = 100
    img_size = 640

    convert_coco = False
    split_coco = False
    DEBUG = False


cfg = Config_manual()




# In[ ]:


# ## Weights

# !mkdir -p {HOME}/weights
# !wget -P {HOME}/weights -q https://download.openmmlab.com/mmyolo/v0/rtmdet/rtmdet_l_syncbn_fast_8xb32-300e_coco/rtmdet_l_syncbn_fast_8xb32-300e_coco_20230102_135928-ee3abdc4.pth
# !ls -lh {HOME}/weights


# In[ ]:


## Configurations Init
CONFIG_PATH = f"{HOME}/mmyolo/configs/rtmdet/rtmdet_l_syncbn_fast_8xb32-300e_coco.py"
WEIGHTS_PATH = f"{HOME}/weights/rtmdet_l_syncbn_fast_8xb32-300e_coco_20230102_135928-ee3abdc4.pth"


# In[ ]:


## Model
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model = init_detector(CONFIG_PATH, WEIGHTS_PATH, device=DEVICE)


# In[ ]:





# ## Prepare COCO 

# In[ ]:


if cfg.convert_coco:
    train = pd.read_csv(cfg.TRAIN_CSV, header=0)
    train = train.dropna() 

    train['bbox'] = train['bbox'].apply(ast.literal_eval)

    # Initialize the COCO format dictionary
    coco_format = {
        "images": [],
        "annotations": [],
        "categories": [
            {"id": 1, "name": "Other"},
            {"id": 2, "name": "Tin"},
            {"id": 3, "name": "Thatch"}
        ]
    }
    index_dict = {}
    i = 0
    unique_image_ids = train['image_id'].unique()
    for image_id in unique_image_ids:
        index_dict[image_id] = i
        i += 1


    # Helper function to get image dimensions
    def get_image_dimensions(image_path):
        with Image.open(image_path) as img:
            return img.width, img.height


    # Convert bbox to segmentation format
    def bbox_to_segmentation(bbox):
        x, y, width, height = bbox
        return [x, y, x + width, y, x + width, y + height, x, y + height]


    # Populate images section
    unique_image_ids = train['image_id'].unique()
    for image_id in unique_image_ids:
        file_name = f"{image_id}.tif"  # Adjust the extension if needed
        image_path = os.path.join(cfg.IMAGE_FOLDER, file_name)
        width, height = get_image_dimensions(image_path)
        coco_format["images"].append({
            "id": index_dict[image_id],
            "file_name": file_name,
            "width": width,
            "height": height
        })

    # Annotations
    for index, row in train.iterrows():
        area = row["bbox"][2] * row["bbox"][3]  # width * height
        if area > 0:
            annotation = {
                "id": index,
                "image_id": index_dict[row["image_id"]],
                "category_id": int(row["category_id"]),
                "bbox": row["bbox"],
                "area": area,
                "iscrowd": 0,
                #"segmentation": []  # Empty if not using segmentation
                "segmentation": [bbox_to_segmentation(row["bbox"])]
            }
            coco_format["annotations"].append(annotation)

    # Save to JSON
    with open(cfg.COCO_DATASET, 'w') as f:
        json.dump(coco_format, f, indent=4)


# ## SPLIT COCO

# In[ ]:


if cfg.split_coco:

    import json
    import os
    import shutil
    import random
    from sklearn.model_selection import train_test_split

    # Function to read the COCO format JSON file
    # Function to read the COCO format JSON file
    def read_coco_json(file_path):
        with open(file_path, 'r') as f:
            data = json.load(f)
        return data

    # Function to split the data while stratifying by class
    def stratified_split(coco_data, test_size=0.1):
        annotations = coco_data['annotations']
        images = coco_data['images']
        categories = coco_data['categories']

        # Create a dictionary mapping image_id to file_name
        image_id_to_filename = {image['id']: image['file_name'].lower() for image in images}

        # Create a list of tuples (image_id, category_id)
        image_id_category_pairs = [(anno['image_id'], anno['category_id']) for anno in annotations]

        # Split the data
        train_ids, valid_ids = train_test_split(
            image_id_category_pairs,
            test_size=test_size,
            stratify=[pair[1] for pair in image_id_category_pairs]
        )

        return train_ids, valid_ids, image_id_to_filename

    # Function to create directories and move files
    def move_files(train_ids, valid_ids, image_id_to_filename, source_dir, dest_dir):
        os.makedirs(os.path.join(dest_dir, 'train'), exist_ok=True)
        os.makedirs(os.path.join(dest_dir, 'valid'), exist_ok=True)

        train_ids_final = []
        valid_ids_final = []

        for image_id, _ in train_ids:
            file_name = image_id_to_filename[image_id]
            source_file = os.path.join(source_dir, file_name)
            if os.path.exists(source_file):
                shutil.move(source_file, os.path.join(dest_dir, 'train', file_name))
                train_ids_final.append((image_id, _))

        for image_id, _ in valid_ids:
            file_name = image_id_to_filename[image_id]
            source_file = os.path.join(source_dir, file_name)
            if os.path.exists(source_file):
                shutil.move(source_file, os.path.join(dest_dir, 'valid', file_name))
                valid_ids_final.append((image_id, _))

        return train_ids_final, valid_ids_final

    # Function to write the COCO JSON file
    def write_coco_json(coco_data, ids, output_file):
        images = [img for img in coco_data['images'] if img['id'] in [id for id, _ in ids]]
        annotations = [anno for anno in coco_data['annotations'] if anno['image_id'] in [id for id, _ in ids]]
        data = {
            'images': images,
            'annotations': annotations,
            'categories': coco_data['categories']
        }
        with open(output_file, 'w') as f:
            json.dump(data, f)





    coco_file_path = cfg.COCO_DATASET
    source_dir = cfg.IMAGE_FOLDER
    dest_dir = cfg.DATA_FOLDER

    # Step 1: Read the COCO format JSON file
    coco_data = read_coco_json(coco_file_path)

    # Step 2: Split the data
    train_ids, valid_ids, image_id_to_filename = stratified_split(coco_data, test_size=0.1)

    # Step 3: Create directories and move files
    train_ids_final, valid_ids_final = move_files(train_ids, valid_ids, image_id_to_filename, source_dir, dest_dir)

    # Step 4: Write the COCO JSON files
    write_coco_json(coco_data, train_ids_final, os.path.join(dest_dir, 'train/_annotations.coco.json'))
    write_coco_json(coco_data, valid_ids_final, os.path.join(dest_dir, 'valid/_annotations.coco.json'))



# ## Validate COCO

# In[ ]:


import json

def validate_coco_json(coco_json):
    # Load JSON
    with open(coco_json, 'r') as f:
        data = json.load(f)

    # Check images
    image_ids = {image['id'] for image in data['images']}
    if not image_ids:
        print("No images found.")
        return False

    # Check categories
    category_ids = {category['id'] for category in data['categories']}
    if not category_ids:
        print("No categories found.")
        return False

    # Check annotations
    for annotation in data['annotations']:
        if annotation['image_id'] not in image_ids:
            print(f"Annotation {annotation['id']} references non-existent image_id {annotation['image_id']}.")
            return False
        if annotation['category_id'] not in category_ids:
            print(f"Annotation {annotation['id']} references non-existent category_id {annotation['category_id']}.")
            return False

        # Check bbox and segmentation
        bbox = annotation['bbox']
        if len(bbox) != 4:
            print(f"Annotation {annotation['id']} has invalid bbox.")
            return False
        segmentation = annotation['segmentation']
        if not isinstance(segmentation, list) or not all(isinstance(seg, list) for seg in segmentation):
            print(f"Annotation {annotation['id']} has invalid segmentation.")
            return False

    print("JSON is valid.")
    return True

# Validate the provided JSON file
validate_coco_json('/notebooks/ZINDI/train/_annotations.coco.json')


# ## Create Custom config Detection

# In[ ]:


print(tuple(sorted(cfg.labels_dict.values())))
print(len(cfg.labels_dict))

#hook_wandb = {'project': project_name}


# In[ ]:


CUSTOM_CONFIG_PATH = f"{HOME}/mmyolo/configs/rtmdet/custom_V1.3.py"

CUSTOM_CONFIG = f"""
_base_ = ['../_base_/default_runtime.py', '../_base_/det_p5_tta.py']

# ========================Frequently modified parameters======================
# -----data related-----
data_root = '{cfg.DATA_FOLDER}/'



resume = True


train_ann_file = 'train/_annotations.coco.json'
train_data_prefix = 'train/'

val_ann_file = 'valid/_annotations.coco.json'
val_data_prefix = 'valid/'

test_ann_file = 'Test_COCO_v2.json'
test_data_prefix = 'Images/'

class_name = {tuple(sorted(cfg.labels_dict.values()))}
num_classes = {len(cfg.labels_dict)}

metainfo = dict(classes=class_name, palette=[(20, 220, 60)])

train_batch_size_per_gpu = {cfg.BATCH_SIZE}
# Worker to pre-fetch data for each single GPU during training
train_num_workers = 8
# persistent_workers must be False if num_workers is 0.
persistent_workers = True

# -----train val related-----
# Base learning rate for optim_wrapper. Corresponding to 8xb16=64 bs
base_lr = 0.004
max_epochs = {cfg.MAX_EPOCHS}  # Maximum training epochs
num_epochs_stage2 = 50

model_test_cfg = dict(
    # The config of multi-label for multi-class prediction.
    multi_label=False,
    # The number of boxes before NMS
    nms_pre=30000,
    score_thr=0.005,  # Threshold to filter out boxes.
    nms=dict(type='nms', iou_threshold=0.65),  # NMS type and threshold
    max_per_img=300)  # Max number of detections of each image

# ========================Possible modified parameters========================
# -----data related-----
img_scale = ({cfg.img_size}, {cfg.img_size})  # width, height
# ratio range for random resize
random_resize_ratio_range = (0.1, 2.0)
# Cached images number in mosaic
mosaic_max_cached_images = 40
# Number of cached images in mixup
mixup_max_cached_images = 20
# Dataset type, this will be used to define the dataset
dataset_type = 'YOLOv5CocoDataset'
# Batch size of a single GPU during validation
val_batch_size_per_gpu = 32
# Worker to pre-fetch data for each single GPU during validation
val_num_workers = 8

# Config of batch shapes. Only on val.
batch_shapes_cfg = dict(
    type='BatchShapePolicy',
    batch_size=val_batch_size_per_gpu,
    img_size=img_scale[0],
    size_divisor=32,
    extra_pad_ratio=0.5)

# -----model related-----
# The scaling factor that controls the depth of the network structure
deepen_factor = 1.0
# The scaling factor that controls the width of the network structure
widen_factor = 1.0
# Strides of multi-scale prior box
strides = [8, 16, 32]

norm_cfg = dict(type='BN')  # Normalization config

# -----train val related-----
lr_start_factor = 1.0e-5
dsl_topk = 13  # Number of bbox selected in each level
loss_cls_weight = 1.0
loss_bbox_weight = 2.0
qfl_beta = 2.0  # beta of QualityFocalLoss
weight_decay = 0.05

# Save model checkpoint and validation intervals
save_checkpoint_intervals = 5
# validation intervals in stage 2
val_interval_stage2 = 1
# The maximum checkpoints to keep.
max_keep_ckpts = 5
# single-scale training is recommended to
# be turned on, which can speed up training.
env_cfg = dict(cudnn_benchmark=True)

# ===============================Unmodified in most cases====================
# https://mmengine.readthedocs.io/en/latest/api/visualization.html
_base_.visualizer.vis_backends = [
    dict(type='LocalVisBackend'), #
    dict(type='TensorboardVisBackend'),]

model = dict(
    type='YOLODetector',
    data_preprocessor=dict(
        type='YOLOv5DetDataPreprocessor',
        mean=[103.53, 116.28, 123.675],
        std=[57.375, 57.12, 58.395],
        bgr_to_rgb=False),
    backbone=dict(
        type='CSPNeXt',
        arch='P5',
        expand_ratio=0.5,
        deepen_factor=deepen_factor,
        widen_factor=widen_factor,
        channel_attention=True,
        norm_cfg=norm_cfg,
        act_cfg=dict(type='SiLU', inplace=True)),
    neck=dict(
        type='CSPNeXtPAFPN',
        deepen_factor=deepen_factor,
        widen_factor=widen_factor,
        in_channels=[256, 512, 1024],
        out_channels=256,
        num_csp_blocks=3,
        expand_ratio=0.5,
        norm_cfg=norm_cfg,
        act_cfg=dict(type='SiLU', inplace=True)),
    bbox_head=dict(
        type='RTMDetHead',
        head_module=dict(
            type='RTMDetSepBNHeadModule',
            num_classes=num_classes,
            in_channels=256,
            stacked_convs=2,
            feat_channels=256,
            norm_cfg=norm_cfg,
            act_cfg=dict(type='SiLU', inplace=True),
            share_conv=True,
            pred_kernel_size=1,
            featmap_strides=strides),
        prior_generator=dict(
            type='mmdet.MlvlPointGenerator', offset=0, strides=strides),
        bbox_coder=dict(type='DistancePointBBoxCoder'),
        loss_cls=dict(
            type='mmdet.QualityFocalLoss',
            use_sigmoid=True,
            beta=qfl_beta,
            loss_weight=loss_cls_weight),
        loss_bbox=dict(type='mmdet.GIoULoss', loss_weight=loss_bbox_weight)),
    train_cfg=dict(
        assigner=dict(
            type='BatchDynamicSoftLabelAssigner',
            num_classes=num_classes,
            topk=dsl_topk,
            iou_calculator=dict(type='mmdet.BboxOverlaps2D')),
        allowed_border=-1,
        pos_weight=-1,
        debug=False),
    test_cfg=model_test_cfg,
)

train_pipeline = [
    dict(type='LoadImageFromFile', backend_args=_base_.backend_args),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='Mosaic',
        img_scale=img_scale,
        use_cached=True,
        max_cached_images=mosaic_max_cached_images,
        pad_val=114.0),
    dict(
        type='mmdet.RandomResize',
        # img_scale is (width, height)
        scale=(img_scale[0] * 2, img_scale[1] * 2),
        ratio_range=random_resize_ratio_range,
        resize_type='mmdet.Resize',
        keep_ratio=True),
    dict(type='mmdet.RandomCrop', crop_size=img_scale),
    dict(type='mmdet.YOLOXHSVRandomAug'),
    dict(type='mmdet.RandomFlip', prob=0.5),
    #dict(type='mmdet.PhotoMetricDistortion'),
    dict(type='mmdet.Pad', size=img_scale, pad_val=dict(img=(114, 114, 114))),
    dict(
        type='YOLOv5MixUp',
        use_cached=True,
        max_cached_images=mixup_max_cached_images),
    dict(type='mmdet.PackDetInputs')
]

train_pipeline_stage2 = [
    dict(type='LoadImageFromFile', backend_args=_base_.backend_args),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='mmdet.RandomResize',
        scale=img_scale,
        ratio_range=random_resize_ratio_range,
        resize_type='mmdet.Resize',
        keep_ratio=True),
    dict(type='mmdet.RandomCrop', crop_size=img_scale),
    dict(type='mmdet.YOLOXHSVRandomAug'),
    dict(type='mmdet.RandomFlip', prob=0.5),
    #dict(type='mmdet.PhotoMetricDistortion'),
    dict(type='mmdet.Pad', size=img_scale, pad_val=dict(img=(114, 114, 114))),
    dict(type='mmdet.PackDetInputs')
]

test_pipeline = [
    dict(type='LoadImageFromFile', backend_args=_base_.backend_args),
    dict(type='YOLOv5KeepRatioResize', scale=img_scale),
    dict(
        type='LetterResize',
        scale=img_scale,
        allow_scale_up=False,
        pad_val=dict(img=114)),
    dict(type='LoadAnnotations', with_bbox=True, _scope_='mmdet'),
    dict(
        type='mmdet.PackDetInputs',
        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                   'scale_factor', 'pad_param'))
]

train_dataloader = dict(
    batch_size=train_batch_size_per_gpu,
    num_workers=train_num_workers,
    persistent_workers=persistent_workers,
    pin_memory=True,
    collate_fn=dict(type='yolov5_collate'),
    sampler=dict(type='DefaultSampler', shuffle=True),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        metainfo=metainfo,
        ann_file=train_ann_file,
        data_prefix=dict(img=train_data_prefix),
        filter_cfg=dict(filter_empty_gt=True, min_size=32),
        pipeline=train_pipeline))

val_dataloader = dict(
    batch_size=val_batch_size_per_gpu,
    num_workers=val_num_workers,
    persistent_workers=persistent_workers,
    pin_memory=True,
    drop_last=False,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        metainfo=metainfo,
        ann_file=val_ann_file,
        data_prefix=dict(img=val_data_prefix),
        test_mode=True,
        batch_shapes_cfg=batch_shapes_cfg,
        pipeline=test_pipeline))

test_dataloader = dict(
    batch_size=val_batch_size_per_gpu,
    num_workers=val_num_workers,
    persistent_workers=persistent_workers,
    pin_memory=True,
    drop_last=False,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        metainfo=metainfo,
        ann_file=test_ann_file,
        data_prefix=dict(img=test_data_prefix),
        test_mode=True,
        batch_shapes_cfg=batch_shapes_cfg,
        pipeline=test_pipeline))

# Reduce evaluation time
val_evaluator = dict(
    type='mmdet.CocoMetric',
    proposal_nums=(100, 1, 10),
    ann_file=data_root + val_ann_file,
    metric='bbox')

test_evaluator = dict(
    type='mmdet.CocoMetric',
    ann_file=data_root + 'Test_COCO_v2.json',
    metric=['bbox'],
    format_only=True, 
    outfile_prefix= data_root +'/InternImage/detection/work_dirs')  


# optimizer
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=weight_decay),
    paramwise_cfg=dict(
        norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))

# learning rate
param_scheduler = [
    dict(
        type='LinearLR',
        start_factor=lr_start_factor,
        by_epoch=False,
        begin=0,
        end=1000),
    dict(
        # use cosine lr from 150 to 300 epoch
        type='CosineAnnealingLR',
        eta_min=base_lr * 0.05,
        begin=max_epochs // 2,
        end=max_epochs,
        T_max=max_epochs // 2,
        by_epoch=True,
        convert_to_iter_based=True),
]

# hooks
default_hooks = dict(
    checkpoint=dict(
        type='CheckpointHook',
        interval=save_checkpoint_intervals,
        max_keep_ckpts=max_keep_ckpts  # only keep latest 3 checkpoints
    ))

custom_hooks = [
    dict(
        type='EMAHook',
        ema_type='ExpMomentumEMA',
        momentum=0.0002,
        update_buffers=True,
        strict_load=False,
        priority=49),
    dict(
        type='mmdet.PipelineSwitchHook',
        switch_epoch=max_epochs - num_epochs_stage2,
        switch_pipeline=train_pipeline_stage2),
    
    

]

train_cfg = dict(
    type='EpochBasedTrainLoop',
    max_epochs=max_epochs,
    val_interval=save_checkpoint_intervals,
    dynamic_intervals=[(max_epochs - num_epochs_stage2, val_interval_stage2)])

val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')
"""
     
with open(CUSTOM_CONFIG_PATH, 'w') as file:
    file.write(CUSTOM_CONFIG)


# ## Terminals

# ### Training

# In[ ]:


get_ipython().run_line_magic('cd', '{HOME}/mmyolo')
get_ipython().system('python tools/train.py configs/rtmdet/custom_V1.3.py')


# ### Inference

# In[ ]:


get_ipython().run_line_magic('cd', '{HOME}/mmyolo')
get_ipython().system('python tools/test.py      /notebooks/ZINDI/mmyolo/configs/rtmdet/custom.py      /notebooks/ZINDI/mmyolo/work_dirs/custom_V1/epoch_100.pth      # --work-dir /notebooks/ZINDI/mmyolo/work_dirs/custom_V1.3      #--cfg-options "jsonfile_prefix=/notebooks/ZINDI/InternImage/detection/work_dirs/results-RTDET-V1.3-EP-70"')


# In[ ]:


# %cd {HOME}/mmyolo
# !python tools/test.py \
#     /notebooks/ZINDI/mmyolo/configs/rtmdet/custom_V1.3.py \
#     /notebooks/ZINDI/mmyolo/work_dirs/custom_V1.3/epoch_70.pth \
#     # --work-dir /notebooks/ZINDI/mmyolo/work_dirs/custom_V1.3 \
#     #--cfg-options "jsonfile_prefix=/notebooks/ZINDI/InternImage/detection/work_dirs/results-RTDET-V1.3-EP-70"


# #### Extract ZINDI from JSON

# In[41]:


import json
import pandas as pd


# Load JSON files
with open(f'/notebooks/ZINDI/InternImage/detection/work_dirs/results-RTDET-V1.0-EP-100.json') as f:
    annotations = json.load(f)

with open('/notebooks/ZINDI/Test_COCO_v2.json') as f:
    image_info = json.load(f)

# Load sample submission file
sample_submission = pd.read_csv('/notebooks/ZINDI/SampleSubmission.csv')



# In[47]:


# Minimum score threshold
threshold = 0.40

# Create a mapping of image_id to file_name
image_id_to_name = {image['id']: image['file_name'] for image in image_info['images']}

# Filter annotations based on the threshold
filtered_annotations = [anno for anno in annotations if anno['score'] >= threshold]

# Prepare the dataframe
data = []
for anno in filtered_annotations:
    file_name = image_id_to_name[anno['image_id']]
    base_name = file_name.rsplit('.', 1)[0]  # Remove file extension
    category_id = anno['category_id']
    data.append({'image_id': base_name, 'category_id': category_id})

df = pd.DataFrame(data)
df['count'] = 1

# Pivot table to get counts of each category for each image
pivot_df = df.pivot_table(index='image_id', columns='category_id', values='count', aggfunc='sum', fill_value=0).reset_index()

# Add missing category columns if not present in pivot table
for category in image_info['categories']:
    if category['id'] not in pivot_df.columns:
        pivot_df[category['id']] = 0

# Create final output with counts
output = []
for _, row in pivot_df.iterrows():
    image_id = row['image_id']
    for category in image_info['categories']:
        category_id = category['id']
        target = row[category_id]
        output.append({'image_id': f"{image_id}_{category_id}", 'Target': target})

# Convert to final dataframe
final_df = pd.DataFrame(output)
final_df.head(10)


# In[48]:


# Left join with sample submission file
merged_df = sample_submission[['image_id']].merge(final_df, on='image_id', how='left')

# Fill NaN values with 0 in 'Target' column
merged_df['Target'] = merged_df['Target'].fillna(0).astype(int)

print(merged_df.Target.sum())
# Display the merged dataframe
merged_df.head(10)


# In[49]:


# Save the final output to a CSV file
merged_df.to_csv(f'/notebooks/RTDET-V-1.0-EP-100-threshold-{threshold}.csv', index=False)


# In[ ]:





# ## Inference Data

# In[ ]:


test = pd.read_csv('/notebooks/ZINDI/Test.csv')
test.head(2)


# In[ ]:


submission  = pd.read_csv('/notebooks/ZINDI/SampleSubmission.csv')
submission.head(2)


# ## Inference Simple Supervision

# In[ ]:


pip install supervision==0.21


# ### SV Direct

# In[ ]:


from mmdet.apis import init_detector, inference_detector
import cv2
import supervision as sv


# In[ ]:





# In[ ]:


config_path = f"{HOME}/mmyolo/work_dirs/custom/custom.py"
model_path = f"{HOME}/mmyolo/work_dirs/custom/epoch_100.pth"

model = init_detector(config_path, model_path, device='cuda:0')


# In[ ]:


#labels_dict = {1: "Other", 2: "Tin", 3: "Thatch" }
labels_dict__ = {3: "Other", 1: "Tin", 2: "Thatch" }


# In[ ]:


detections[0]


# In[ ]:


target_ = 'id_arrv98g89a1z'
image_file = f"/notebooks/ZINDI/train/{target_}.tif" # image or folder path
image = cv2.imread(image_file)

result = inference_detector(model, image)
detections = sv.Detections.from_mmdetection(result)
detections = detections[detections.confidence > 0.4].with_nms()

box_annotator = sv.BoxAnnotator()


labels = [
    f"{labels_dict__[class_id]} {confidence:0.2f}"
    for _, _, confidence, class_id, _ , _
    in detections
]

counter = {'Other': 0 , 'Tin': 0 , 'Thatch': 0 ,}
for _, _, confidence, class_id, _, _ in detections:
    counter[labels_dict__[class_id]] += 1

annotated_image = box_annotator.annotate(image.copy(), detections, labels=labels)
print(counter)
sv.plot_image(image=annotated_image, size=(10, 10))


# In[ ]:


# ds = sv.DetectionDataset.from_coco(
#     images_directory_path=f"{dataset.location}/test",
#     annotations_path=f"{dataset.location}/test/_annotations.coco.json",
# )

# images = list(ds.images.values())


# ### SV Slicer

# In[ ]:


import supervision as sv
import numpy as np
from supervision.detection.tools import inference_slicer
import cv2
from mmdet.apis import init_detector, inference_detector


# In[ ]:


from mmyolo.utils import register_all_modules
register_all_modules()


# In[ ]:


HOME = '/notebooks/ZINDI'
print("HOME:", HOME)


# In[ ]:





# In[ ]:


config_path = f"{HOME}/mmyolo/work_dirs/custom/custom_V1.3.py"
model_path = f"{HOME}/mmyolo/work_dirs/custom/epoch_100.pth"

model = init_detector(config_path, model_path, device='cuda:0')


# In[ ]:


'''
Thres: 0.50:
    - Count > 40
        Thres: 0.45
            - Count >= 50
                Final Thres: == 0.40
            Else:
                Thres = Same @ 0.45
    

Eliminate by BBOX Area
< Threshold of False Positive,,, Collect False Positive, Get Mean of Area, 
Eliminate by area Less

'''


# In[ ]:


target_ = 'id_0hp558k5xkss'
image_file = f"/notebooks/ZINDI/train/{target_}.tif" # image or folder path
image = cv2.imread(image_file)


# In[ ]:


## RAW

result = inference_detector(model, image)
#result.get('pred_instances')

results = result.get('pred_instances')
ms = results['scores'] >= 0.40
results['scores'][ms]
print(len(results['scores'][ms]))


# In[ ]:


from mmengine.structures import InstanceData
CONFIDENCE_THRESHOLD = 0.40
NMS_IOU_THRESHOLD = 0.05
area_flag = False
area_thres = 500

def callback(image_slice: np.ndarray) -> sv.Detections:
    #results = inference_detector(model, image_slice).get('pred_instances')#[0]
    #results = model.infer(image_slice)[0]
    #return sv.Detections.from_inference(results)

    result = inference_detector(model, image)

    if area_flag:
        # Calculate areas
        bboxes = result.get('pred_instances')['bboxes']
        areas = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])
        result.get('pred_instances')['areas'] = areas

        results = result.get('pred_instances')

        # Create a mask to filter elements based on the area threshold
        mask = results['areas'] >= area_thres

        # Create an InstanceData object and assign the filtered data to it
        filtered_instance_data = InstanceData(
            bboxes=results['bboxes'][mask],
            scores=results['scores'][mask],
            labels=results['labels'][mask],
            areas=results['areas'][mask]
        )
        result.pred_instances = filtered_instance_data
    detections = sv.Detections.from_mmdetection(result)
    #print(type(result))
    clean = detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)
    
    return clean


slicer = inference_slicer.InferenceSlicer(callback=callback, slice_wh= (640,640), overlap_ratio_wh = (0.05, 0.05), iou_threshold = NMS_IOU_THRESHOLD)
sliced_detections = slicer(image=image)


labels_dict__ = {3: "Other", 1: "Tin", 2: "Thatch", 0 : "Background" }

counter = {'Other': 0 , 'Tin': 0 , 'Thatch': 0 , "Background" : 0}
for clss in sliced_detections.class_id:
    counter[labels_dict__[clss]] += 1
print(counter)


# In[ ]:


# CONFIDENCE_THRESHOLD = 0.50
# NMS_IOU_THRESHOLD = 0.20

# def callback(image_slice: np.ndarray) -> sv.Detections:

#     result = inference_detector(model, image)
#     detections = sv.Detections.from_mmdetection(result)
#     print(type(result))
#     clean = detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)
#     print(clean)
#     return clean


# slicer = inference_slicer.InferenceSlicer(callback=callback, slice_wh= (4000,4000), overlap_ratio_wh = (0.50, 0.50), iou_threshold = NMS_IOU_THRESHOLD)
# sliced_detections = slicer(image=image)


# labels_dict__ = {3: "Other", 1: "Tin", 2: "Thatch", 0 : "Background" }

# counter = {'Other': 0 , 'Tin': 0 , 'Thatch': 0 , "Background" : 0}
# for clss in sliced_detections.class_id:
#     counter[labels_dict__[clss]] += 1
# print(counter)


# In[ ]:


label_annotator = sv.LabelAnnotator()
box_annotator = sv.BoxAnnotator()

# You can also use sv.MaskAnnotator() for instance segmentation models
# mask_annotator = sv.MaskAnnotator()

annotated_image = box_annotator.annotate(
    scene=image.copy(), detections=sliced_detections)
# annotated_image = mask_annotator.annotate(
#    scene=image.copy(), detections=sliced_detections)

annotated_image = label_annotator.annotate(
    scene=annotated_image, detections=sliced_detections)

sv.plot_image(annotated_image)


# ### VALIDATE DS Performance

# In[ ]:


ds = sv.DetectionDataset.from_coco(
    images_directory_path=f"{HOME}/train",
    annotations_path=f"{HOME}/train/_annotations.coco.json",
)

print('dataset classes:', ds.classes)
print('dataset size:', len(ds))


# In[ ]:


def callback(image: np.ndarray) -> sv.Detections:
    result = inference_detector(model, image)
    detections = sv.Detections.from_mmdetection(result)
    return detections[detections.confidence > CONFIDENCE_THRESHOLD].with_nms(threshold=NMS_IOU_THRESHOLD)


# In[ ]:


CONFIDENCE_THRESHOLD = 0.50
NMS_IOU_THRESHOLD = 0.60

confusion_matrix = sv.ConfusionMatrix.benchmark(
    dataset = ds,
    callback = callback
)

#_ = confusion_matrix.plot()
     


# In[ ]:


_ = confusion_matrix.plot()


# In[ ]:


CONFIDENCE_THRESHOLD = 0.45
NMS_IOU_THRESHOLD = 0.60

confusion_matrix = sv.ConfusionMatrix.benchmark(
    dataset = ds,
    callback = callback
)

_ = confusion_matrix.plot()
     


# In[ ]:


CONFIDENCE_THRESHOLD = 0.40
NMS_IOU_THRESHOLD = 0.60

confusion_matrix = sv.ConfusionMatrix.benchmark(
    dataset = ds,
    callback = callback
)

_ = confusion_matrix.plot()
     


# In[ ]:


CONFIDENCE_THRESHOLD = 0.60
NMS_IOU_THRESHOLD = 0.60

confusion_matrix = sv.ConfusionMatrix.benchmark(
    dataset = ds,
    callback = callback
)

_ = confusion_matrix.plot()
     


# ## Inference SAHI

# In[ ]:


#!pip install -U sahi mmdet mmengine 'mmcv>=2.0.0'


# In[ ]:


#!pip install mmyolo==0.1.2


# In[ ]:


#!pip install sahi==0.11.16


# In[ ]:


from sahi import AutoDetectionModel
from sahi.utils.cv import read_image
from sahi.predict import get_prediction, get_sliced_prediction, predict
from IPython.display import Image


# In[ ]:


from mmyolo.utils import register_all_modules
register_all_modules()


# In[ ]:


HOME = '/notebooks/ZINDI'
print("HOME:", HOME)


# In[ ]:


from mmengine.registry import DefaultScope


#config_path = f"{HOME}/mmyolo/configs/rtmdet/custom.py"

config_path = f"{HOME}/mmyolo/work_dirs/custom/custom_.py"
model_path = f"{HOME}/mmyolo/work_dirs/custom/epoch_100.pth"

#model = init_detector(config_path, model_path, device='cuda:0')

# with DefaultScope.overwrite_default_scope('mmdet'):
#     predictor = init_detector(config_path, model_path)


# In[ ]:


# get batch predict result
target_ = 'id_bw60pzuanqn5'
result = predict(
    model_type='mmdet', # one of 'yolov5', 'mmdet', 'detectron2'
    model_path= model_path, # path to model weight file
    model_config_path=config_path, # for detectron2 and mmdet models
    model_confidence_threshold=0.5,
    model_device='cuda:0', # or 'cuda:0'
    source=f"/notebooks/ZINDI/Images/{target_}.tif", # image or folder path
    no_standard_prediction=True,
    no_sliced_prediction=False,
    slice_height=320,
    slice_width=320,
    overlap_height_ratio=0.5,
    overlap_width_ratio=0.5,
    export_pickle=False,
    export_crop=False,
)


# In[ ]:


detection_model = AutoDetectionModel.from_pretrained(
    model_type='mmdet',
    model_path=model_path,
    config_path=config_path,
    confidence_threshold=0.4,
    image_size=640,
    device='cuda:0', # or ('cuda:0', 'cpu')
)


# In[ ]:


# Display Single Predictions Without SAHI
from IPython.display import Image

target_ = 'id_bw60pzuanqn5'
result = get_prediction(f"/notebooks/ZINDI/Images/{target_}.tif", detection_model)
result.export_visuals(export_dir="/notebooks/ZINDI/inference_demo/")
Image(f"/notebooks/ZINDI/inference_demo/{target_}.png")


# #### SAHI

# In[ ]:





# In[ ]:




