# %%
!pip install ensemble_boxes

# %%
import json
import cv2
import numpy as np
import matplotlib.pyplot as plt
from ensemble_boxes import weighted_boxes_fusion
from collections import defaultdict


# %% [markdown]
# ### Filter Small BBOX

# %%
# def filter_small_boxes(detections, box_too_small_factor):
#     filtered_detections = []

#     for detection in detections:
#         bboxes = detection.get('bbox', [])
#         classes = detection.get('class', [])
#         scores = detection.get('score', [1.0] * len(bboxes))

#         # if len(bboxes) == 0:
#         #     filtered_detections.append(detection)
#         #     continue

#         # Ensure all bboxes are in the correct format
#         bboxes = [box for box in bboxes if isinstance(box, (list, tuple)) and len(box) == 4]

#         # if not bboxes:
#         #     filtered_detections.append(detection)
#         #     continue

#         box_areas = [box[2] * box[3] for box in bboxes]
#         largest_area = max(box_areas)
#         box_idx = 0

#         while box_idx < len(bboxes):
#             if box_areas[box_idx] < largest_area * box_too_small_factor:
#                 del box_areas[box_idx]
#                 bboxes.pop(box_idx)
#                 classes.pop(box_idx)
#                 scores.pop(box_idx)
#             else:
#                 box_idx += 1

#         filtered_detections.append({
#             'image_id': detection['image_id'],
#             'bbox': bboxes,
#             'class': classes,
#             'score': scores
#         })

#     return filtered_detections

def filter_small_boxes(detections, box_too_small_factor):
    """
    Filter out bounding boxes that are too small compared to the largest bounding box in the image.

    Args:
        detections (list of dict): List of detection results for each image. Each dict should contain 'bbox' and 'category_id' keys.
        box_too_small_factor (float): Factor to determine the size threshold for small boxes.

    Returns:
        list of dict: Filtered detection results.
    """
    from collections import defaultdict

    # Group detections by image_id
    grouped_detections = defaultdict(list)
    for detection in detections:
        grouped_detections[detection['image_id']].append(detection)

    filtered_detections = []

    for image_id, image_detections in grouped_detections.items():
        bboxes = [d['bbox'] for d in image_detections]
        if len(bboxes) == 0:
            continue

        box_areas = [box[2] * box[3] for box in bboxes]  # Assuming bbox format is [x, y, width, height]
        largest_area = max(box_areas)

        for detection, area in zip(image_detections, box_areas):
            if area >= largest_area * box_too_small_factor:
                filtered_detections.append(detection)

    return filtered_detections


# %% [markdown]
# ## WBF Ensemble

# %%
def ensemble_bboxes_wbf(detections, iou_threshold=0.5):
    ensembled_detections = []

    for detection in detections:
        bboxes = detection['bbox']
        classes = detection['class']
        scores = detection['score']

        # if len(bboxes) == 0:
        #     ensembled_detections.append(detection)
        #     continue

        boxes_list = [[bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]] for bbox in bboxes]
        labels_list = classes
        scores_list = scores
        boxes, scores, labels = weighted_boxes_fusion([boxes_list], [scores_list], [labels_list], iou_thr=iou_threshold)

        ensembled_bboxes = [[box[0], box[1], box[2] - box[0], box[3] - box[1]] for box in boxes]
        ensembled_detections.append({
            'image_id': detection['image_id'],
            'bbox': ensembled_bboxes,
            'class': labels.tolist(),
            'score': scores.tolist()
        })

    return ensembled_detections


# %% [markdown]
# ## Process List of Detections MMDET

# %%
def process_and_ensemble(input_json_files, box_too_small_factor=0.025, iou_threshold=0.5, output_json_file=None):
    all_detections = defaultdict(list)

    for input_json_file in input_json_files:
        with open(input_json_file, 'r') as file:
            detections_list = json.load(file)
            for detection in detections_list:
                image_id = detection['image_id']
                all_detections[image_id].append(detection)

    ensembled_detections = []

    for image_id, detections in all_detections.items():
        filtered_detections = filter_small_boxes(detections, box_too_small_factor)
        ensembled_detections.extend(ensemble_bboxes_wbf(filtered_detections, iou_threshold))

    if output_json_file:
        with open(output_json_file, 'w') as file:
            json.dump(ensembled_detections, file, indent=4)

    return ensembled_detections



# %%
## RUN

input_json_files = ['/notebooks/ZINDI/mmyolo/yolov8_TTA-S-LOW_IOU.json']

output_json_file = 'filtered_ensemble_detections-V3.json'
box_too_small_factor = 0.025
iou_threshold = 0.5

det = process_and_ensemble(input_json_files, box_too_small_factor, iou_threshold)


# %% [markdown]
# ## Utils

# %% [markdown]
# ### BBOXES Size

# %%
def determine_image_size(detections):
    """
    Determine the size of the image from the bounding boxes.
    
    Args:
        detections (list of dict): Detection results containing bounding boxes.
    
    Returns:
        tuple: Image size (width, height).
    """
    max_x = max(det['bbox'][0] + det['bbox'][2] for det in detections)
    max_y = max(det['bbox'][1] + det['bbox'][3] for det in detections)
    return (int(max_x), int(max_y))


 # input_ = '/notebooks/ZINDI/mmdet_outputs/results-Internimage-Best-EP-34.bbox.json'   | 1000 * 1000
# input_ = '/notebooks/ZINDI/mmdet_outputs/results-InternImage-Best-EP-8.bbox.json'     | 1000 * 1000
# input_ = '/notebooks/ZINDI/mmdet_outputs/results-RTDET-Ep-310.json'                     | 1000 * 1000
input_ = '/notebooks/ZINDI/mmyolo/results-yolo8-epoch300-TTA.json'

with open(input_, 'r') as file:
    detections_list_ = json.load(file)

determine_image_size(detections_list_)

# %% [markdown]
# ### Visualize

# %%
import os
import random
import json
import cv2
import matplotlib.pyplot as plt

# %%
def visualize_detections(image_dir, detections=None, detections_file=None):
    """
    Visualize the detection results on a randomly selected image from a directory.

    Args:
        image_dir (str): Path to the directory containing images.
        detections (list of dict, optional): Detection results to visualize. If None, detections_file must be provided.
        detections_file (str, optional): Path to the JSON file containing detection results. If None, detections must be provided.
    """
    if detections is None and detections_file is None:
        raise ValueError("Either 'detections' or 'detections_file' must be provided")

    if detections is None:
        with open(detections_file, 'r') as file:
            detections = json.load(file)

    # Randomly select an image from the directory
    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]
    if not image_files:
        raise ValueError("No image files found in the specified directory")
    
    selected_image = random.choice(image_files)
    image_path = os.path.join(image_dir, selected_image)
    
    # Filter detections for the selected image
    image_id = int(os.path.splitext(selected_image)[0])  # Assuming image filenames are their respective image_ids
    image_detections = [det for det in detections if det['image_id'] == image_id]

    if not image_detections:
        raise ValueError(f"No detections found for the selected image ID: {image_id}")

    image = cv2.imread(image_path)
    for detection in image_detections:
        for bbox in detection['bbox']:
            x, y, w, h = map(int, bbox)
            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

    
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.show()



# %%
image_dir = ''
detections_         = None
detections_file_    = None
visualize_detections(image_dir, detections = detections_, detections_file=detections_file_ )


# %% [markdown]
# ## Convert To Zindi

# %%
import json
import pandas as pd

P1 = '/notebooks/ZINDI/mmyolo/Pseudo-V2-Regular-Test.json'
P2 = '/notebooks/ZINDI/mmyolo/Pseudo-V2-TTA-Test.json'
P3 = '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU15-320.json'
P4 = '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU15-1920.json'

N1 = '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU15-1240-Direct-MultiLabel.json' # | TH:0.45  Count:8596 OR 0.40 & Filter = 9K
N2 = '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU40-1240-Direct-SingleLabel.json' #                       OR 0.40 & Filter = 8984
N3 = '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU10-1240-Direct-SingleLabel.json' #                       OR 0.40 & Filter = 8984

# Load JSON files
with open(N3) as f:
    annotations = json.load(f)

with open('/notebooks/ZINDI/Test_COCO_v2.json') as f:
    image_info = json.load(f)

# Load sample submission file
sample_submission = pd.read_csv('/notebooks/ZINDI/SampleSubmission.csv')



# %%
low_size_factor = 0.04
annotations = filter_small_boxes(annotations, low_size_factor)

# %%

# Minimum score threshold
threshold = 0.40

# Create a mapping of image_id to file_name
image_id_to_name = {image['id']: image['file_name'] for image in image_info['images']}

# Filter annotations based on the threshold
filtered_annotations = [anno for anno in annotations if anno['score'] >= threshold]

# Prepare the dataframe
data = []
for anno in filtered_annotations:
    file_name = image_id_to_name[anno['image_id']]
    base_name = file_name.rsplit('.', 1)[0]  # Remove file extension
    category_id = anno['category_id']
    data.append({'image_id': base_name, 'category_id': category_id})

df = pd.DataFrame(data)
df['count'] = 1

# Pivot table to get counts of each category for each image
pivot_df = df.pivot_table(index='image_id', columns='category_id', values='count', aggfunc='sum', fill_value=0).reset_index()

# Add missing category columns if not present in pivot table
for category in image_info['categories']:
    if category['id'] not in pivot_df.columns:
        pivot_df[category['id']] = 0

# Create final output with counts
output = []
for _, row in pivot_df.iterrows():
    image_id = row['image_id']
    for category in image_info['categories']:
        category_id = category['id']
        target = row[category_id]
        output.append({'image_id': f"{image_id}_{category_id}", 'Target': target})

# Convert to final dataframe
final_df = pd.DataFrame(output)
print(final_df.Target.sum())
final_df.head(10)

# %%
# import numpy as np
# final_df['Target'] = np.round(final_df['Target'] / 100)  
# print(final_df.Target.sum())
# final_df

# %%

# Left join with sample submission file
merged_df = sample_submission[['image_id']].merge(final_df, on='image_id', how='left')

# Fill NaN values with 0 in 'Target' column
merged_df['Target'] = merged_df['Target'].fillna(0).astype(int)

print(merged_df.Target.sum())
# Display the merged dataframe
merged_df.head(10)

# %%
# Save the final output to a CSV file
merged_df.to_csv(f'/notebooks/PSEUDOV2-N3Trial-threshold-{threshold}+++++++++++++++++++++++++.csv', index=False)

# %%
N1 = '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU15-1240-Direct-MultiLabel.json' # | TH:0.45  Count:8596 OR 0.40 & Filter = 9K
N2 = '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU40-1240-Direct-SingleLabel.json' #                       OR 0.40 & Filter = 8984
N3 = '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU10-1240-Direct-SingleLabel.json' #                       OR 0.40 & Filter = 8984


# %% [markdown]
# ## Fuse

# %%
%cd /notebooks/ZINDI/mmdetection
!python tools/analysis_tools/fuse_results.py \
    '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU15-1240-Direct-MultiLabel.json' \
    '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU40-1240-Direct-SingleLabel.json' \
    '/notebooks/ZINDI/mmyolo/PSUEDO2-IOU10-1240-Direct-SingleLabel.json' \
    --weights 1 1 1 \
    --save_fusion_results \
    --out_dir /notebooks/ZINDI/mmdet_outputs
        


# %%



